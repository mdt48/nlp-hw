{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install [_Miniconda_](https://docs.conda.io/en/main/miniconda.html) or [_Anaconda_](https://docs.anaconda.com/anaconda/install/index.html)\n",
    "2. Create a new virtual Python environment: <code>conda create -n gwnlp Python=3.10</code>\n",
    "3. Activate your environment (and you'll use this Python environment throughout the course - make sure it is selected as the Python interpreter if you are using an IDE like VS Code): <code>conda activate gwnlp</code>\n",
    "4. Install packages (this will give you pandas, pytorch, fastai, spacy, etc.): <code>conda install -c fastchan fastai</code>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (20 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a (5 points). Normalize all of the raw phone numbers with Python RE module. Find one pattern that works for all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Raw | Normalized |\n",
    "| --- | --- |\n",
    "| 2021213121 | +1 (202) 121-3121 |\n",
    "| 12021213121 | +1 (202) 121-3121 |\n",
    "| +12021213121 | +1 (202) 121-3121 |\n",
    "| 202-121-3121 | +1 (202) 121-3121 |\n",
    "| (202)  121 -   3121 | +1 (202) 121-3121 |\n",
    "| (202)121-3121 | +1 (202) 121-3121 |\n",
    "| 862021213121 | +86 (202) 121-3121 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'\\d+'\n",
    "\n",
    "numbers = ['2021213121', '12021213121', '+12021213121', '202-121-3121', '(202)  121 -   3121', '(202)121-3121', '862021213121']\n",
    "\n",
    "\n",
    "\n",
    "for number in numbers:\n",
    "    # print(re.match(pattern, number).group())\n",
    "    stripped_number = ''\n",
    "    for i in re.finditer(pattern, number):\n",
    "        stripped_number+=i.string[i.regs[0][0]:i.regs[0][1]]\n",
    "\n",
    "    n = len(stripped_number)\n",
    "    print(number, stripped_number)\n",
    "    \n",
    "    difference = n - 10\n",
    "\n",
    "    country_code = stripped_number[:difference]\n",
    "    area_code = stripped_number[difference:difference+3]\n",
    "\n",
    "    first_group = stripped_number[difference+3:difference+6]\n",
    "    second_group = stripped_number[difference+6:]\n",
    "\n",
    "    phone_number_string = '+{}({}) {}-{}'.format(1, area_code, first_group, second_group)\n",
    "    print('\\t' +phone_number_string)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b (15 points). Use Python RE module to complete the following tasks, with **one** regex pattern **for each**. Show your test samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add spaces around / and #. E.g., \"good/bad\" -> \"good / bad\".\n",
    "2. Replace tokens in ALL CAPS by their lower version. E.g., \"This is AMAZING!\" -> \"This is amazing!\".\n",
    "3. Convert _camel case_ to _snake case_. E.g., \"getNamesFromUserInput\" -> \"get_names_from_user_input\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "r = r'/'\n",
    "print(re.sub(r, ' / ', 'good/bad'))\n",
    "\n",
    "# 2\n",
    "r = r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b'\n",
    "print(re.sub(r, lambda m: m.group(0).lower(), 'This is AMAZING!'))\n",
    "\n",
    "# 3\n",
    "r = r'[^a-z]+'\n",
    "print(re.sub(r, lambda m: '_'+m.group(0).lower(), 'getNamesFromUserInput'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: For Problem 2 - 5 we will work on a sample of IMDB Reviews dataset. Load the data into a _pandas_ _Dataframe_ (review [the basics of pandas](https://pandas.pydata.org/docs/user_guide/10min.html) if you are new to it) using the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fastai.data.external import URLs, untar_data\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 800, 200, 476, 524)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), sum(df['is_valid'] == False), sum(df['is_valid'] == True), sum(df['label'] == 'positive'), sum(df['label'] == 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a (5 points). \n",
    "- Find at least one thing that needs to be cleaned with regex in the texts. Show your Python code.\n",
    "- Create train/valid split using the column 'is_valid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_fix = '\"This\", is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries.'\n",
    "\n",
    "# remove <br /> and any punctuation\n",
    "pattern = r'\\.*<\\s*br\\s*\\/>|[\".,\\/#!$%\\^&\\*;:{}=\\-_`~()]'\n",
    "re.sub(pattern, '', string_to_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df.loc[df['is_valid']]\n",
    "df_train = df.loc[df['is_valid'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(to_replace=pattern, value='', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b (5 points). \n",
    "- Implement your own tokenizer for the texts. Requirements: split by space, remove most punctuations and split common abbreviations (e.g., \"don't\" -> \"do\" \"n't\", \"you'll\" -> \"you\" \"'ll\"). \n",
    "- Create 3 vocabularies using top 1000, 5000, and 10000 tokens, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def remove_contraction(text):\n",
    "    text = re.sub(r'n\\'t', ' not', text)\n",
    "    text = re.sub(r'\\'re', ' are', text)\n",
    "    text = re.sub(r'\\'s', ' is', text)\n",
    "    text = re.sub(r'\\'d', ' would', text)\n",
    "    text = re.sub(r'\\'ll', 'will', text)\n",
    "    text = re.sub(r'\\'t', ' not', text)\n",
    "    text = re.sub(r'\\'ve', ' have', text)\n",
    "    text = re.sub(r'\\'m', ' am', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def word_count(words):\n",
    "    count = defaultdict(int)\n",
    "\n",
    "    for word in words:\n",
    "        count[word] += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def clean_text(df, column):\n",
    "    # REMOVE PUNCTUATION\n",
    "    pattern = r'\\.*<\\s*br\\s*\\/>|[\".,\\/#!$%\\^&\\*;:{}=\\-_`~()]'\n",
    "\n",
    "    tokenized_df = df.replace(to_replace=pattern, value='', regex=True)\n",
    "\n",
    "    # LOWERCASE\n",
    "    tokenized_df[column] = tokenized_df[column].apply(str.lower)\n",
    "\n",
    "    # contractions\n",
    "    tokenized_df[column] = tokenized_df[column].apply(remove_contraction)\n",
    "\n",
    "    return tokenized_df\n",
    "\n",
    "def tokenizer(df, column):\n",
    "    # words\n",
    "    sentences = df[column].to_list()\n",
    "    sentences = ' '.join(sentences)\n",
    "    words = sentences.split(' ') \n",
    "    \n",
    "    word_counts = word_count(words)\n",
    "    sorted_word_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    ret = pd.DataFrame(columns=['word', 'freq', 'positive', 'negative', 'freq_pos', 'freq_neg'])\n",
    "    \n",
    "    ret['word'] = list(sorted_word_counts.keys())\n",
    "    ret['freq'] = list(sorted_word_counts.values())\n",
    "\n",
    "    # check if in pos class\n",
    "    sentences = df.loc[df['label'] == 'positive'][column].to_list()\n",
    "    sentences = ' '.join(sentences)\n",
    "    words = sentences.split(' ') \n",
    "    pos = []\n",
    "    freqs =[]\n",
    "    for word in ret['word'].to_list():\n",
    "        c = words.count(word)\n",
    "        freq = c\n",
    "        if c >0: \n",
    "            pos.append(True)\n",
    "        else:\n",
    "            pos.append(False)\n",
    "        freqs.append(freq)\n",
    "    ret['freq_pos'] = freqs\n",
    "    \n",
    "    \n",
    "    # check if in neg class\n",
    "    sentences = df.loc[df['label'] == 'negative'][column].to_list()\n",
    "    sentences = ' '.join(sentences)\n",
    "    words = sentences.split(' ') \n",
    "    \n",
    "    neg = []\n",
    "    freqs =[]\n",
    "\n",
    "    for word in ret['word'].to_list():\n",
    "        c = words.count(word)\n",
    "        freq = c\n",
    "        if c >0: \n",
    "            neg.append(True)\n",
    "        else:\n",
    "            neg.append(False)\n",
    "        freqs.append(freq)\n",
    "    ret['freq_neg'] = freqs\n",
    "\n",
    "    ret['positive'] = pos\n",
    "    ret['negative'] = neg\n",
    "    return ret\n",
    "    # return list(sorted_word_counts)[:len_vocab]\n",
    "\n",
    "df_train, df_valid = clean_text(df_train, 'text'), clean_text(df_valid, 'text')\n",
    "word_counts = tokenizer(df_train, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>freq_pos</th>\n",
       "      <th>freq_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>3002</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1441</td>\n",
       "      <td>1561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>1483</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>722</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>1455</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>725</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>1442</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>664</td>\n",
       "      <td>778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>1366</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>673</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  freq  positive  negative  freq_pos  freq_neg\n",
       "0  the  3002      True      True      1441      1561\n",
       "1   is  1483      True      True       722       761\n",
       "2  and  1455      True      True       725       730\n",
       "3    a  1442      True      True       664       778\n",
       "4   of  1366      True      True       673       693"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c (5 points). \n",
    "- Implement on your own and train a Naive Bayes sentiment classifier in the _training set_. Requirements: use log scales and add-one smoothing.\n",
    "- Report your model performances on the _validation set_, with the 3 vocabs your created in 2b, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def build_l_table(df, len_vocab):\n",
    "    liklihood_table = defaultdict(dict)\n",
    "\n",
    "    top_n = df.iloc[:len_vocab, :]\n",
    "\n",
    "    total_pos = top_n['freq_pos'].sum()\n",
    "    total_neg = top_n['freq_neg'].sum()\n",
    "\n",
    "    for word in tqdm(top_n['word'], desc='building liklihood table'):\n",
    "        pos_lik = int(top_n.loc[top_n['word'] == word]['freq_pos'])\n",
    "        neg_lik = int(top_n.loc[top_n['word'] == word]['freq_neg'])\n",
    "\n",
    "        liklihood_table[word]['pos'] = np.log((pos_lik + 1) / (total_pos + len_vocab))\n",
    "        liklihood_table[word]['neg'] = np.log((neg_lik + 1) / (total_neg + len_vocab))\n",
    "    \n",
    "    return liklihood_table, (total_pos, total_neg)\n",
    "\n",
    "\n",
    "\n",
    "def priors(df):\n",
    "    N = len(df)\n",
    "\n",
    "    positive_prior = df['label'].value_counts()['positive'] / N\n",
    "    negative_prior = df['label'].value_counts()['negative'] / N\n",
    "\n",
    "    return np.log(positive_prior), np.log(negative_prior)\n",
    "\n",
    "def nb_preprocess(df_train, word_counts, len_vocab):\n",
    "    p_prior, n_prior = priors(df_train)\n",
    "\n",
    "    likelihood_table, total_for_classes = build_l_table(word_counts, len_vocab)\n",
    "\n",
    "    return (p_prior, n_prior), likelihood_table, total_for_classes\n",
    "\n",
    "def naive_bayes(priors, likelihoods, total_for_classes, eval_sentences):\n",
    "    preds = []\n",
    "\n",
    "    p_prior, n_prior = priors\n",
    "\n",
    "\n",
    "    for sentence in tqdm(eval_sentences):\n",
    "\n",
    "        pos_log_lik, neg_log_lik = 0, 0\n",
    "        for word in sentence.split(' '):\n",
    "            if word not in list(likelihoods.keys()):\n",
    "                pos_log_lik += np.log(1 / total_for_classes[0])\n",
    "                neg_log_lik += np.log(1 / total_for_classes[1])\n",
    "            else:\n",
    "                if 'pos' not in list(likelihoods[word].keys()):\n",
    "                    pos_log_lik += np.log(1 / total_for_classes[0])\n",
    "                else:\n",
    "                    pos_log_lik += likelihoods[word]['pos']\n",
    "\n",
    "                if 'neg' not in list(likelihoods[word].keys()):\n",
    "                    neg_log_lik += np.log(1 / total_for_classes[1])\n",
    "                else:\n",
    "                    neg_log_lik += likelihoods[word]['neg']\n",
    "\n",
    "        \n",
    "        pos_pred = p_prior + pos_log_lik\n",
    "        neg_pred = n_prior + neg_log_lik\n",
    "\n",
    "        pred = 'positive' if pos_pred > neg_pred else 'negative'\n",
    "\n",
    "        preds.append(pred)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building liklihood table: 100%|██████████| 1000/1000 [00:00<00:00, 3610.41it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 619.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, l, total_for_classes = nb_preprocess(df_train, word_counts, 1000)\n",
    "preds = naive_bayes(p, l, total_for_classes, df_valid['text'].to_list())\n",
    "\n",
    "accuracy_score(df_valid['label'].to_list(), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building liklihood table: 100%|██████████| 5000/5000 [00:02<00:00, 1909.95it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 143.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, l, total_for_classes = nb_preprocess(df_train, word_counts, 5000)\n",
    "preds = naive_bayes(p, l, total_for_classes, df_valid['text'].to_list())\n",
    "\n",
    "accuracy_score(df_valid['label'].to_list(), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building liklihood table: 100%|██████████| 8639/8639 [00:06<00:00, 1399.01it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 89.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, l, total_for_classes = nb_preprocess(df_train, word_counts, 10000)\n",
    "preds = naive_bayes(p, l, total_for_classes, df_valid['text'].to_list())\n",
    "\n",
    "accuracy_score(df_valid['label'].to_list(), preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d (5 points). Use [_spaCy_](https://spacy.io/) to _tokenize_ and _lemmatize_ this time. Get a new vocab of top 10000 lemmas. Retrain your model on this vocab and report its performance on the validation set.\n",
    "(Note that spaCy relies on language-specific databases to work. Even though it is already importable, you still need to install its dependency for English. If you are in your _jupyter notebook_, create a new cell and execute the following: <code>!python -m spacy download en_core_web_sm</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (20 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a (10 points). \n",
    "- Implement your own _subword tokenizer_ (the algorithm can be found in the slides). \n",
    "- Create 3 vocabularies of size 1000, 5000, and 10000, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b (5 points). Compare the number of unknown words in your training set between the 3 tokenizers and 3 subword tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c (5 points). Train your Naive Bayes classifier with the subword tokenizer of 10000 tokens. Compare your model performance (better/worse/same?) and give your analysis (why)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (20 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a (10 points). Build two probabilistic language models using 2-gram and 3-gram, respectively, on the _entire_ texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b (10 points). Generate 5 examples for each of the LM. Compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 (20 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a (10 points). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run topic modeling with SVD for 2, 6, and 10 topics, respectively.\n",
    "- Extract 10 keywords for each topic.\n",
    "- Try to mannually assign topic labels for (some of) them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b (5 points)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "- Remove stopwords\n",
    "- Lemmatize\n",
    "- Keep only nouns, verbs, and adjs with the help of spaCy POS tagger\n",
    "- Remove certain named entities (choose whatever makes sense to you)\n",
    "- Remove html tags\n",
    "- Remove non-ascii characters\n",
    "\n",
    "And run SVD again for 10 topics. Compare your results with 5a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c (5 points). Find 2 most similar pairs of reviews using document embeddings derived from SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwnlo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af8207cabd4e670ce08470e4167faab7783bfd687a13b41d452e716ef3a3118e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
