{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install [_Miniconda_](https://docs.conda.io/en/main/miniconda.html) or [_Anaconda_](https://docs.anaconda.com/anaconda/install/index.html)\n",
    "2. Create a new virtual Python environment: <code>conda create -n gwnlp Python=3.10</code>\n",
    "3. Activate your environment (and you'll use this Python environment throughout the course - make sure it is selected as the Python interpreter if you are using an IDE like VS Code): <code>conda activate gwnlp</code>\n",
    "4. Install packages (this will give you pandas, pytorch, fastai, spacy, etc.): <code>conda install -c fastchan fastai</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (20 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a (5 points). Normalize all of the raw phone numbers with Python RE module. Find one pattern that works for all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Raw | Normalized |\n",
    "| --- | --- |\n",
    "| 2021213121 | +1 (202) 121-3121 |\n",
    "| 12021213121 | +1 (202) 121-3121 |\n",
    "| +12021213121 | +1 (202) 121-3121 |\n",
    "| 202-121-3121 | +1 (202) 121-3121 |\n",
    "| (202)  121 -   3121 | +1 (202) 121-3121 |\n",
    "| (202)121-3121 | +1 (202) 121-3121 |\n",
    "| 862021213121 | +86 (202) 121-3121 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021213121 2021213121\n",
      "\t+1(202) 121-3121\n",
      "12021213121 12021213121\n",
      "\t+1(202) 121-3121\n",
      "+12021213121 12021213121\n",
      "\t+1(202) 121-3121\n",
      "202-121-3121 2021213121\n",
      "\t+1(202) 121-3121\n",
      "(202)  121 -   3121 2021213121\n",
      "\t+1(202) 121-3121\n",
      "(202)121-3121 2021213121\n",
      "\t+1(202) 121-3121\n",
      "862021213121 862021213121\n",
      "\t+1(202) 121-3121\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'\\d+'\n",
    "\n",
    "numbers = ['2021213121', '12021213121', '+12021213121', '202-121-3121', '(202)  121 -   3121', '(202)121-3121', '862021213121']\n",
    "\n",
    "\n",
    "\n",
    "for number in numbers:\n",
    "    # print(re.match(pattern, number).group())\n",
    "    stripped_number = ''\n",
    "    for i in re.finditer(pattern, number):\n",
    "        stripped_number+=i.string[i.regs[0][0]:i.regs[0][1]]\n",
    "\n",
    "    n = len(stripped_number)\n",
    "    print(number, stripped_number)\n",
    "    \n",
    "    difference = n - 10\n",
    "\n",
    "    country_code = stripped_number[:difference]\n",
    "    area_code = stripped_number[difference:difference+3]\n",
    "\n",
    "    first_group = stripped_number[difference+3:difference+6]\n",
    "    second_group = stripped_number[difference+6:]\n",
    "\n",
    "    phone_number_string = '+{}({}) {}-{}'.format(1, area_code, first_group, second_group)\n",
    "    print('\\t' +phone_number_string)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b (15 points). Use Python RE module to complete the following tasks, with **one** regex pattern **for each**. Show your test samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add spaces around / and #. E.g., \"good/bad\" -> \"good / bad\".\n",
    "2. Replace tokens in ALL CAPS by their lower version. E.g., \"This is AMAZING!\" -> \"This is amazing!\".\n",
    "3. Convert _camel case_ to _snake case_. E.g., \"getNamesFromUserInput\" -> \"get_names_from_user_input\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good / bad\n",
      "This is amazing!\n",
      "get_names_from_user_input\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "\n",
    "r = r'/'\n",
    "print(re.sub(r, ' / ', 'good/bad'))\n",
    "\n",
    "# 2\n",
    "r = r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b'\n",
    "print(re.sub(r, lambda m: m.group(0).lower(), 'This is AMAZING!'))\n",
    "\n",
    "# 3\n",
    "r = r'[^a-z]+'\n",
    "print(re.sub(r, lambda m: '_'+m.group(0).lower(), 'getNamesFromUserInput'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: For Problem 2 - 5 we will work on a sample of IMDB Reviews dataset. Load the data into a _pandas_ _Dataframe_ (review [the basics of pandas](https://pandas.pydata.org/docs/user_guide/10min.html) if you are new to it) using the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fastai.data.external import URLs, untar_data\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 800, 200, 476, 524)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), sum(df['is_valid'] == False), sum(df['is_valid'] == True), sum(df['label'] == 'positive'), sum(df['label'] == 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a (5 points). \n",
    "- Find at least one thing that needs to be cleaned with regex in the texts. Show your Python code.\n",
    "- Create train/valid split using the column 'is_valid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a extremely wellmade film The acting script and camerawork are all firstrate The music is good too though it is mostly early in the film when things are still relatively cheery There are no really superstars in the cast though several faces will be familiar The entire cast does an excellent job with the scriptBut it is hard to watch because there is no good end to a situation like the one presented It is now fashionable to blame the British for setting Hindus and Muslims against each other and then cruelly separating them into two countries'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_fix = '\"This\", is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries.'\n",
    "\n",
    "# remove <br /> and any punctuation\n",
    "pattern = r'\\.*<\\s*br\\s*\\/>|[\".,\\/#!$%\\^&\\*;:{}=\\-_`~()]'\n",
    "re.sub(pattern, '', string_to_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df.loc[df['is_valid'] == True]\n",
    "df_train = df.loc[df['is_valid'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>unbleepingbelievable meg ryan does not even look her usual pert lovable self in this which normally makes me forgive her shallow ticky acting schtick hard to believe she was the producer on this dog plus kevin kline what kind of suicide trip has his career been on? whoosh banzai finally this was directed by the guy who did big chill? must be a replay of jonestown  hollywood style wooofff</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>this is a extremely wellmade film the acting script and camerawork are all firstrate the music is good too though it is mostly early in the film when things are still relatively cheery there are no really superstars in the cast though several faces will be familiar the entire cast does an excellent job with the scriptbut it is hard to watch because there is no good end to a situation like the one presented it is now fashionable to blame the british for setting hindus and muslims against each other and then cruelly separating them into two countries there is some merit in this view but it i...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>every once in a long while a movie will come along that will be so awful that i feel compelled to warn people if i labor all my days and i can save but one soul from watching this movie how great will be my joywhere to begin my discussion of pain for starters there was a musical montage every five minutes there was no character development every character was a stereotype we had swearing guy fat guy who eats donuts goofy foreign guy etc the script felt as if it were being written as the movie was being shot the production value was so incredibly low that it felt like i was watching a junio...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>name just says it all i watched this movie with my dad when it came out and having served in korea he had great admiration for the man the disappointing thing about this film is that it only concentrate on a short period of the man is life  interestingly enough the man is entire life would have made such an epic biopic that it is staggering to imagine the cost for productionsome posters elude to the flawed characteristics about the man which are cheap shots the theme of the movie duty honor country are not just mere words blathered from the lips of a highbrassed officer  it is the deep dec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>this movie succeeds at being one of the most unique movies you have seen however this comes from the fact that you ca not make heads or tails of this mess it almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid if you do not want to feel slighted youwill sit through this horrible film and develop a real sense of pity for the actors involved they have all seen better days but then you realize they actually got paid quite a bit of money to do this and youwill lose pity for them just like you hav...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                                   unbleepingbelievable meg ryan does not even look her usual pert lovable self in this which normally makes me forgive her shallow ticky acting schtick hard to believe she was the producer on this dog plus kevin kline what kind of suicide trip has his career been on? whoosh banzai finally this was directed by the guy who did big chill? must be a replay of jonestown  hollywood style wooofff   \n",
       "1  this is a extremely wellmade film the acting script and camerawork are all firstrate the music is good too though it is mostly early in the film when things are still relatively cheery there are no really superstars in the cast though several faces will be familiar the entire cast does an excellent job with the scriptbut it is hard to watch because there is no good end to a situation like the one presented it is now fashionable to blame the british for setting hindus and muslims against each other and then cruelly separating them into two countries there is some merit in this view but it i...   \n",
       "2  every once in a long while a movie will come along that will be so awful that i feel compelled to warn people if i labor all my days and i can save but one soul from watching this movie how great will be my joywhere to begin my discussion of pain for starters there was a musical montage every five minutes there was no character development every character was a stereotype we had swearing guy fat guy who eats donuts goofy foreign guy etc the script felt as if it were being written as the movie was being shot the production value was so incredibly low that it felt like i was watching a junio...   \n",
       "3  name just says it all i watched this movie with my dad when it came out and having served in korea he had great admiration for the man the disappointing thing about this film is that it only concentrate on a short period of the man is life  interestingly enough the man is entire life would have made such an epic biopic that it is staggering to imagine the cost for productionsome posters elude to the flawed characteristics about the man which are cheap shots the theme of the movie duty honor country are not just mere words blathered from the lips of a highbrassed officer  it is the deep dec...   \n",
       "4  this movie succeeds at being one of the most unique movies you have seen however this comes from the fact that you ca not make heads or tails of this mess it almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid if you do not want to feel slighted youwill sit through this horrible film and develop a real sense of pity for the actors involved they have all seen better days but then you realize they actually got paid quite a bit of money to do this and youwill lose pity for them just like you hav...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.replace(to_replace=pattern, value='', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b (5 points). \n",
    "- Implement your own tokenizer for the texts. Requirements: split by space, remove most punctuations and split common abbreviations (e.g., \"don't\" -> \"do\" \"n't\", \"you'll\" -> \"you\" \"'ll\"). \n",
    "- Create 3 vocabularies using top 1000, 5000, and 10000 tokens, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def remove_contraction(text):\n",
    "    text = re.sub(r'n\\'t', ' not', text)\n",
    "    text = re.sub(r'\\'re', ' are', text)\n",
    "    text = re.sub(r'\\'s', ' is', text)\n",
    "    text = re.sub(r'\\'d', ' would', text)\n",
    "    text = re.sub(r'\\'ll', 'will', text)\n",
    "    text = re.sub(r'\\'t', ' not', text)\n",
    "    text = re.sub(r'\\'ve', ' have', text)\n",
    "    text = re.sub(r'\\'m', ' am', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def word_count(words):\n",
    "    count = defaultdict(int)\n",
    "\n",
    "    for word in words:\n",
    "        count[word] += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def clean_text(df, column):\n",
    "    # REMOVE PUNCTUATION\n",
    "    pattern = r'\\.*<\\s*br\\s*\\/>|[\".,\\/#!$%\\^&\\*;:{}=\\-_`~()]'\n",
    "\n",
    "    tokenized_df = df.replace(to_replace=pattern, value='', regex=True)\n",
    "\n",
    "    # LOWERCASE\n",
    "    tokenized_df[column] = tokenized_df[column].apply(str.lower)\n",
    "\n",
    "    # contractions\n",
    "    tokenized_df[column] = tokenized_df[column].apply(remove_contraction)\n",
    "\n",
    "    return tokenized_df\n",
    "\n",
    "def tokenizer(df, column):\n",
    "    # words\n",
    "    sentences = df[column].to_list()\n",
    "    sentences = ' '.join(sentences)\n",
    "    words = sentences.split(' ') \n",
    "    \n",
    "    word_counts = word_count(words)\n",
    "    sorted_word_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    ret = pd.DataFrame(columns=['word', 'freq', 'positive', 'negative', 'freq_pos', 'freq_neg'])\n",
    "    \n",
    "    ret['word'] = list(sorted_word_counts.keys())\n",
    "    ret['freq'] = list(sorted_word_counts.values())\n",
    "\n",
    "    # check if in pos class\n",
    "    sentences = df.loc[df['label'] == 'positive'][column].to_list()\n",
    "    sentences = ' '.join(sentences)\n",
    "    words = sentences.split(' ') \n",
    "    pos = []\n",
    "    freqs =[]\n",
    "    for word in ret['word'].to_list():\n",
    "        c = words.count(word)\n",
    "        freq = c\n",
    "        if c >0: \n",
    "            pos.append(True)\n",
    "        else:\n",
    "            pos.append(False)\n",
    "        freqs.append(freq)\n",
    "    ret['freq_pos'] = freqs\n",
    "    \n",
    "    \n",
    "    # check if in neg class\n",
    "    sentences = df.loc[df['label'] == 'negative'][column].to_list()\n",
    "    sentences = ' '.join(sentences)\n",
    "    words = sentences.split(' ') \n",
    "    \n",
    "    neg = []\n",
    "    freqs =[]\n",
    "\n",
    "    for word in ret['word'].to_list():\n",
    "        c = words.count(word)\n",
    "        freq = c\n",
    "        if c >0: \n",
    "            neg.append(True)\n",
    "        else:\n",
    "            neg.append(False)\n",
    "        freqs.append(freq)\n",
    "    ret['freq_neg'] = freqs\n",
    "\n",
    "    ret['positive'] = pos\n",
    "    ret['negative'] = neg\n",
    "    return ret\n",
    "    # return list(sorted_word_counts)[:len_vocab]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = clean_text(df_train, 'text'), clean_text(df_valid, 'text')\n",
    "\n",
    "word_counts = tokenizer(df_train, 'text')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c (5 points). \n",
    "- Implement on your own and train a Naive Bayes sentiment classifier in the _training set_. Requirements: use log scales and add-one smoothing.\n",
    "- Report your model performances on the _validation set_, with the 3 vocabs your created in 2b, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict\n",
    "def build_l_table(df, len_vocab):\n",
    "    liklihood_table = defaultdict(dict)\n",
    "\n",
    "    top_n = df.iloc[:len_vocab, :]\n",
    "\n",
    "    total_pos = top_n['freq_pos'].sum()\n",
    "    total_neg = top_n['freq_neg'].sum()\n",
    "\n",
    "    for word in tqdm(top_n['word'], desc='building liklihood table'):\n",
    "        pos_lik = int(top_n.loc[top_n['word'] == word]['freq_pos'])\n",
    "        neg_lik = int(top_n.loc[top_n['word'] == word]['freq_neg'])\n",
    "\n",
    "        liklihood_table[word]['pos'] = np.log((pos_lik + 1) / (total_pos + len_vocab))\n",
    "        liklihood_table[word]['neg'] = np.log((neg_lik + 1) / (total_neg + len_vocab))\n",
    "    \n",
    "    return liklihood_table, (total_pos, total_neg)\n",
    "\n",
    "\n",
    "\n",
    "def priors(df):\n",
    "    N = len(df)\n",
    "\n",
    "    positive_prior = df['label'].value_counts()['positive'] / N\n",
    "    negative_prior = df['label'].value_counts()['negative'] / N\n",
    "\n",
    "    return np.log(positive_prior), np.log(negative_prior)\n",
    "\n",
    "def nb_preprocess(df_train, word_counts, len_vocab):\n",
    "    p_prior, n_prior = priors(df_train)\n",
    "\n",
    "    likelihood_table, total_for_classes = build_l_table(word_counts, len_vocab)\n",
    "\n",
    "    return (p_prior, n_prior), likelihood_table, total_for_classes\n",
    "\n",
    "def naive_bayes(priors, likelihoods, total_for_classes, eval_sentences):\n",
    "    preds = []\n",
    "\n",
    "    p_prior, n_prior = priors\n",
    "\n",
    "\n",
    "    for sentence in tqdm(eval_sentences):\n",
    "\n",
    "        pos_log_lik, neg_log_lik = 0, 0\n",
    "        for word in sentence.split(' '):\n",
    "            if word not in list(likelihoods.keys()):\n",
    "                pos_log_lik += np.log(1 / total_for_classes[0])\n",
    "                neg_log_lik += np.log(1 / total_for_classes[1])\n",
    "            else:\n",
    "                if 'pos' not in list(likelihoods[word].keys()):\n",
    "                    pos_log_lik += np.log(1 / total_for_classes[0])\n",
    "                else:\n",
    "                    pos_log_lik += likelihoods[word]['pos']\n",
    "\n",
    "                if 'neg' not in list(likelihoods[word].keys()):\n",
    "                    neg_log_lik += np.log(1 / total_for_classes[1])\n",
    "                else:\n",
    "                    neg_log_lik += likelihoods[word]['neg']\n",
    "\n",
    "        \n",
    "        pos_pred = p_prior + pos_log_lik\n",
    "        neg_pred = n_prior + neg_log_lik\n",
    "\n",
    "        pred = 'positive' if pos_pred > neg_pred else 'negative'\n",
    "\n",
    "        preds.append(pred)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building liklihood table: 100%|██████████| 1000/1000 [00:00<00:00, 1760.64it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 566.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.695"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, l, total_for_classes = nb_preprocess(df_train, word_counts, 1000)\n",
    "preds = naive_bayes(p, l, total_for_classes, df_valid['text'].to_list())\n",
    "\n",
    "accuracy_score(df_valid['label'].to_list(), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building liklihood table: 100%|██████████| 5000/5000 [00:04<00:00, 1199.11it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 132.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, l, total_for_classes = nb_preprocess(df_train, word_counts, 5000)\n",
    "preds = naive_bayes(p, l, total_for_classes, df_valid['text'].to_list())\n",
    "\n",
    "accuracy_score(df_valid['label'].to_list(), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building liklihood table: 100%|██████████| 10000/10000 [00:11<00:00, 835.99it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 71.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, l, total_for_classes = nb_preprocess(df_train, word_counts, 10000)\n",
    "preds = naive_bayes(p, l, total_for_classes, df_valid['text'].to_list())\n",
    "\n",
    "accuracy_score(df_valid['label'].to_list(), preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d (5 points). Use [_spaCy_](https://spacy.io/) to _tokenize_ and _lemmatize_ this time. Get a new vocab of top 10000 lemmas. Retrain your model on this vocab and report its performance on the validation set.\n",
    "(Note that spaCy relies on language-specific databases to work. Even though it is already importable, you still need to install its dependency for English. If you are in your _jupyter notebook_, create a new cell and execute the following: <code>!python -m spacy download en_core_web_sm</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'texts.csv')\n",
    "\n",
    "df_valid = df.loc[df['is_valid']]\n",
    "df_train = df.loc[df['is_valid'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def frequency(words):\n",
    "    res = {}\n",
    " \n",
    "    for keys in words:\n",
    "        res[keys] = res.get(keys, 0) + 1\n",
    "    return res\n",
    "\n",
    "def get_lemmas(df, label):\n",
    "    lemmas = []\n",
    "    for i in range(len(df.loc[df['label'] == label])):\n",
    "        doc = nlp(df.iloc[i, 1])\n",
    "\n",
    "        for token in doc:\n",
    "            if not token.is_stop:\n",
    "                lemmas.append(token.lemma_.lower())\n",
    "    return lemmas\n",
    "\n",
    "    \n",
    "def get_lemmas_valid(text):\n",
    "    lemmas = []\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            lemmas.append(token.lemma_.lower())\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "    \n",
    "\n",
    "def spacy_tokenizer(df):\n",
    "    ret = pd.DataFrame(columns=['word', 'freq', 'positive', 'negative', 'freq_pos', 'freq_neg'])\n",
    "\n",
    "    pos_lemmas = get_lemmas(df, 'positive')\n",
    "    neg_lemmas = get_lemmas(df, 'negative')\n",
    "\n",
    "    pos_freq_dict = frequency(list(pos_lemmas))\n",
    "    neg_freq_dict = frequency(list(neg_lemmas))\n",
    "\n",
    "    words = set(pos_lemmas).union(set(neg_lemmas))\n",
    "\n",
    "    for word in words:\n",
    "        if word not in pos_freq_dict:\n",
    "            pos_freq = 0\n",
    "        else:\n",
    "            pos_freq = pos_freq_dict[word]\n",
    "\n",
    "        if word not in neg_freq_dict:\n",
    "            neg_freq = 0\n",
    "        else:\n",
    "            neg_freq = neg_freq_dict[word] \n",
    "        row = {\n",
    "            'word': word,\n",
    "            'freq':  pos_freq + neg_freq,\n",
    "            'positive': True if word in pos_freq_dict else False,\n",
    "            'negative': True if word in neg_freq_dict else False,\n",
    "            'freq_pos': pos_freq,\n",
    "            'freq_neg': neg_freq\n",
    "        }\n",
    "\n",
    "        ret = ret.append(row, ignore_index=True);\n",
    "\n",
    "    return ret\n",
    "    # print(pos_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = spacy_tokenizer(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/6cp5ymss2v568_lm1k7xzxf00000gn/T/ipykernel_23590/2186989555.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_valid['text'] = df_valid['text'].apply(get_lemmas_valid)\n"
     ]
    }
   ],
   "source": [
    "df_valid['text'] = df_valid['text'].apply(get_lemmas_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building liklihood table: 100%|██████████| 10000/10000 [00:10<00:00, 931.67it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 74.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, l, total_for_classes = nb_preprocess(df_train, word_counts, 10000)\n",
    "preds = naive_bayes(p, l, total_for_classes, df_valid['text'].to_list())\n",
    "\n",
    "accuracy_score(df_valid['label'].to_list(), preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastai.data.external import URLs, untar_data\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'texts.csv')\n",
    "df = clean_text(df, 'text')\n",
    "df_valid = df.loc[df['is_valid']]\n",
    "df_train = df.loc[df['is_valid'] == True]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a (10 points). \n",
    "- Implement your own _subword tokenizer_ (the algorithm can be found in the slides). \n",
    "- Create 3 vocabularies of size 1000, 5000, and 10000, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 10000\n"
     ]
    }
   ],
   "source": [
    "def most_common_pair(tokens):\n",
    "    occurences = defaultdict(int)\n",
    "    for i in range(len(tokens) - 1):\n",
    "        occurences[(tokens[i] , tokens[i+1])] += 1\n",
    "    sorted_occurences = {k: v for k, v in sorted(occurences.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    return list(sorted_occurences.keys())[0]\n",
    "\n",
    "def bpe_tokenizer(strings, target_vocab_size):\n",
    "    V = []\n",
    "    for string in strings:\n",
    "        # V.union(set(string.replace(' ', '')))\n",
    "        V.extend(list(string.replace(' ', '')))\n",
    "        \n",
    "    V = list(set(V))\n",
    "    C = list(' '.join(strings))\n",
    "    while len(V) < target_vocab_size:\n",
    "        tL, tR = most_common_pair(C)\n",
    "\n",
    "        tNew = tL + tR\n",
    "\n",
    "        tNew = tNew\n",
    "        V.append(tNew)\n",
    "\n",
    "        C_new = []\n",
    "\n",
    "        i = 0\n",
    "        while i < len(C)-1:\n",
    "            if C[i] == tL and C[i+1] == tR:\n",
    "                C_new.append(tNew)\n",
    "                i += 2\n",
    "            else:\n",
    "                C_new.append(C[i])\n",
    "                i+=1\n",
    "            # strings[idx] = ' '.join(s)\n",
    "        C = C_new\n",
    "        print(\"\\r\\rVocab Size = {}\".format(len(V)), end='')\n",
    "    print()\n",
    "    return V\n",
    "\n",
    "# V_1000 = bpe_tokenizer(df_train['text'].to_list(), target_vocab_size=1000)\n",
    "# V_5000 = bpe_tokenizer(df_train['text'].to_list(), target_vocab_size=5000)\n",
    "V_10000 = bpe_tokenizer(df_train['text'].to_list(), target_vocab_size=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b (5 points). Compare the number of unknown words in your training set between the 3 tokenizers and 3 subword tokenizers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c (5 points). Train your Naive Bayes classifier with the subword tokenizer of 10000 tokens. Compare your model performance (better/worse/same?) and give your analysis (why)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc(df, column, vocab):\n",
    "    ret = pd.DataFrame(columns=['word', 'positive', 'negative', 'freq_pos', 'freq_neg'])\n",
    "    \n",
    "    ret['word'] = vocab\n",
    "\n",
    "\n",
    "    # check if in pos class\n",
    "    sentences = df.loc[df['label'] == 'positive'][column].to_list()\n",
    "    sentences = ' '.join(sentences)\n",
    "    words = sentences.split(' ')\n",
    "\n",
    "    freqs =[]\n",
    "    pos = []\n",
    "\n",
    "    for v in vocab:\n",
    "        c = words.count(v)\n",
    "        freq = c\n",
    "        if c >0: \n",
    "            pos.append(True)\n",
    "        else:\n",
    "            pos.append(False)\n",
    "        freqs.append(freq)\n",
    "    ret['freq_pos'] = freqs\n",
    "    \n",
    "    \n",
    "    # check if in neg class\n",
    "    sentences = df.loc[df['label'] == 'negative'][column].to_list()\n",
    "    sentences = ' '.join(sentences)\n",
    "    words = sentences.split(' ') \n",
    "    \n",
    "    neg = []\n",
    "    freqs =[]\n",
    "\n",
    "    for v in vocab:\n",
    "        c = words.count(v)\n",
    "        freq = c\n",
    "        if c >0: \n",
    "            neg.append(True)\n",
    "        else:\n",
    "            neg.append(False)\n",
    "        freqs.append(freq)\n",
    "    ret['freq_neg'] = freqs\n",
    "\n",
    "    ret['positive'] = pos\n",
    "    ret['negative'] = neg\n",
    "    return ret\n",
    "\n",
    "word_counts = wc(df_train, 'text', V_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building liklihood table: 100%|██████████| 1000/1000 [00:00<00:00, 1699.73it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 499.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.635"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, l, total_for_classes = nb_preprocess(df_train, word_counts, 1000)\n",
    "preds = naive_bayes(p, l, total_for_classes, df_valid['text'].to_list())\n",
    "\n",
    "accuracy_score(df_valid['label'].to_list(), preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Answer\n",
    "\n",
    "## Results\n",
    "\n",
    "For 10000 word vocabs these were my results:\n",
    "\n",
    "- My own Tokenizer: 77\n",
    "- Spacy:            55\n",
    "- BPE:              63.5\n",
    "\n",
    "## Analysis\n",
    "\n",
    "I think my tokenizer had better quality data, leading to better results, for a few reasons: \n",
    "1) I was able to remove punctuation better. Especially for Spacy tokenization things like HTML tags were still left in the strings.\n",
    "2) Neither broke apart contractions. I think having separate words for contractions helped a lot\n",
    "3) I was not really sure how to use spacy properly! I think I did it right, but I was definitely not a master of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (20 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a (10 points). Build two probabilistic language models using 2-gram and 3-gram, respectively, on the _entire_ texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fastai.data.external import URLs, untar_data\n",
    "import string\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "df = clean_text(df, 'text')\n",
    "# df_train = df.loc[df['is_valid'] == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set(' '.join(df['text'].to_list()).split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "fitted_cv = cv.fit_transform(df['text'].to_list())\n",
    "freq_df = pd.DataFrame(fitted_cv.toarray(), columns = cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['text'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>007 game</th>\n",
       "      <th>00s that</th>\n",
       "      <th>010ps what</th>\n",
       "      <th>03oct2009 observed</th>\n",
       "      <th>07 knew</th>\n",
       "      <th>081006 with</th>\n",
       "      <th>10 are</th>\n",
       "      <th>10 based</th>\n",
       "      <th>10 because</th>\n",
       "      <th>10 but</th>\n",
       "      <th>...</th>\n",
       "      <th>zooms frantically</th>\n",
       "      <th>zucco all</th>\n",
       "      <th>zuccopreparing his</th>\n",
       "      <th>zulu later</th>\n",
       "      <th>zulu people</th>\n",
       "      <th>zulu to</th>\n",
       "      <th>zulus without</th>\n",
       "      <th>zuniga of</th>\n",
       "      <th>zvyagvatsev 2003</th>\n",
       "      <th>über alles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 103807 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     007 game  00s that  010ps what  03oct2009 observed  07 knew  081006 with  \\\n",
       "0           0         0           0                   0        0            0   \n",
       "1           0         0           0                   0        0            0   \n",
       "2           0         0           0                   0        0            0   \n",
       "3           0         0           0                   0        0            0   \n",
       "4           0         0           0                   0        0            0   \n",
       "..        ...       ...         ...                 ...      ...          ...   \n",
       "795         0         0           0                   0        0            0   \n",
       "796         0         0           0                   0        0            0   \n",
       "797         0         0           0                   0        0            0   \n",
       "798         0         0           0                   0        0            0   \n",
       "799         0         0           0                   0        0            0   \n",
       "\n",
       "     10 are  10 based  10 because  10 but  ...  zooms frantically  zucco all  \\\n",
       "0         0         0           0       0  ...                  0          0   \n",
       "1         0         0           0       0  ...                  0          0   \n",
       "2         0         0           0       0  ...                  0          0   \n",
       "3         0         0           0       0  ...                  0          0   \n",
       "4         0         0           0       0  ...                  0          0   \n",
       "..      ...       ...         ...     ...  ...                ...        ...   \n",
       "795       0         0           0       0  ...                  0          0   \n",
       "796       0         0           0       0  ...                  0          0   \n",
       "797       0         0           0       0  ...                  0          0   \n",
       "798       0         0           0       0  ...                  0          0   \n",
       "799       0         0           0       0  ...                  0          0   \n",
       "\n",
       "     zuccopreparing his  zulu later  zulu people  zulu to  zulus without  \\\n",
       "0                     0           0            0        0              0   \n",
       "1                     0           0            0        0              0   \n",
       "2                     0           0            0        0              0   \n",
       "3                     0           0            0        0              0   \n",
       "4                     0           0            0        0              0   \n",
       "..                  ...         ...          ...      ...            ...   \n",
       "795                   0           0            0        0              0   \n",
       "796                   0           0            0        0              0   \n",
       "797                   0           0            0        0              0   \n",
       "798                   0           0            0        0              0   \n",
       "799                   0           0            0        0              0   \n",
       "\n",
       "     zuniga of  zvyagvatsev 2003  über alles  \n",
       "0            0                 0           0  \n",
       "1            0                 0           0  \n",
       "2            0                 0           0  \n",
       "3            0                 0           0  \n",
       "4            0                 0           0  \n",
       "..         ...               ...         ...  \n",
       "795          0                 0           1  \n",
       "796          0                 0           0  \n",
       "797          0                 0           0  \n",
       "798          0                 0           0  \n",
       "799          0                 0           0  \n",
       "\n",
       "[800 rows x 103807 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def bi_prior_hist(freq_df, history):\n",
    "    # probability of the history\n",
    "    # pr = c(0, 1)/ c(0) * c(1, 2) / c(1)...\n",
    "    \n",
    "    # biggram history\n",
    "    pr = 1\n",
    "    for i in range(0, len(history) - 1):\n",
    "        g = history[i] + ' ' + history[i+1]\n",
    "\n",
    "        occurences_of_i = 0\n",
    "        for c in freq_df.columns:\n",
    "            if history[i] in c:\n",
    "                occurences_of_i += freq_df[c].sum()\n",
    "        if g in freq_df.columns:\n",
    "            pr *= freq_df[g].sum() / occurences_of_i\n",
    "            # pr *= df[g].sum(axis=1) / occurences_of_i\n",
    "    return pr\n",
    "\n",
    "def bi_gram(freq_df, history, word, pr):\n",
    "    g = history[-1] + ' ' + word\n",
    "    occurences_of_i = 0\n",
    "    for c in freq_df.columns:\n",
    "        if history[-1] in c:\n",
    "            occurences_of_i += freq_df[c].sum()\n",
    "    if g in freq_df.columns:\n",
    "        pr *= freq_df[g].sum() / occurences_of_i\n",
    "\n",
    "    return pr\n",
    "\n",
    "def bi_gram_model(all_words, freq_df, history, n):\n",
    "\n",
    "    ret = ' '.join(history)\n",
    "\n",
    "    pr = bi_prior_hist(freq_df, history)\n",
    "    for i in tqdm(range(n)):\n",
    "        m_prob, m_word = 0, ''\n",
    "\n",
    "        for word in all_words:\n",
    "            pr = bi_gram(freq_df, history, word, pr)\n",
    "            if pr > m_prob:\n",
    "                m_word = word\n",
    "                m_prob = pr\n",
    "        return ret + m_word\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b (10 points). Generate 5 examples for each of the LM. Compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bi_gram_model(all_words, freq_df, ['this', 'movie', 'is'], 10))\n",
    "print(bi_gram_model(all_words, freq_df, ['the', 'english', 'man'], 10))\n",
    "print(bi_gram_model(all_words, freq_df, ['zulu', 'people', 'are'], 10))\n",
    "print(bi_gram_model(all_words, freq_df, ['bad', 'movie'], 10))\n",
    "print(bi_gram_model(all_words, freq_df, ['bad', 'acting', 'man'], 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 (20 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a (10 points). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run topic modeling with SVD for 2, 6, and 10 topics, respectively.\n",
    "- Extract 10 keywords for each topic.\n",
    "- Try to mannually assign topic labels for (some of) them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastai.data.external import URLs, untar_data\n",
    "import string\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "df = df.loc[df['is_valid'] == False]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import decomposition\n",
    "from scipy import linalg\n",
    "from sklearn.feature_extraction import _stop_words as stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(smooth_idf=True)\n",
    "vectors = vectorizer.fit_transform(df['text'].to_list()).todense()\n",
    "vocab = np.array(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(comps, n_topics):\n",
    "    topic_word_list = []\n",
    "    for i, comp in enumerate(comps):\n",
    "        terms_comp = zip(vocab,comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_topics]\n",
    "        topic=\" \"\n",
    "        for t in sorted_terms:\n",
    "            topic= topic + ' ' + t[0]\n",
    "        topic_word_list.append(topic)\n",
    "        # print(topic_word_list)\n",
    "    return topic_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  the and of br to it is in that this',\n",
       " '  br she her his episode he of as woman by',\n",
       " '  br movie it you was this bad my so if',\n",
       " '  the was movie were of series bad worst br plot',\n",
       " '  was he she her his had were didn in to',\n",
       " '  he movie his you the is bad man him if',\n",
       " '  movie and her she is good very story this acting',\n",
       " '  she her you is the where if not to but',\n",
       " '  film is he this was bad it very films the',\n",
       " '  you film was this will to love the my see']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = 10\n",
    "svd = decomposition.TruncatedSVD(n_components=n_topics, algorithm='randomized')\n",
    "\n",
    "svd.fit(np.asarray(vectors))\n",
    "components = svd.components_\n",
    "print_topics(components, n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  the and of br to it',\n",
       " '  br she her his episode he',\n",
       " '  br movie it you was this',\n",
       " '  he his is to and her',\n",
       " '  was he she her his had',\n",
       " '  film and it her she would']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = 6\n",
    "svd = decomposition.TruncatedSVD(n_components=n_topics, algorithm='randomized')\n",
    "\n",
    "svd.fit(np.asarray(vectors))\n",
    "components = svd.components_\n",
    "print_topics(components, n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  the and', '  br she']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = 2\n",
    "svd = decomposition.TruncatedSVD(n_components=n_topics, algorithm='randomized')\n",
    "\n",
    "svd.fit(np.asarray(vectors))\n",
    "components = svd.components_\n",
    "print_topics(components, n_topics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5A\n",
    "It is hard to assign topics that make sense to this. The most distinguishable one is that the movie is bad. However, it is confused by the stop words and other distracting characters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b (5 points)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "- Remove stopwords\n",
    "- Lemmatize\n",
    "- Keep only nouns, verbs, and adjs with the help of spaCy POS tagger\n",
    "- Remove certain named entities (choose whatever makes sense to you)\n",
    "- Remove html tags\n",
    "- Remove non-ascii characters\n",
    "\n",
    "And run SVD again for 10 topics. Compare your results with 5a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "df = df.loc[df['is_valid'] == False]\n",
    "df = clean_text(df, 'text') # clean removes tags and non ascii!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_remove(text):\n",
    "    \"\"\"\n",
    "    WILL REMOVE PUNCT, STOPS, AND LEAVE ONLY NOUN, VERB, ADJ\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    new_text = \"\"\n",
    "    for token in doc:\n",
    "        if not token.is_stop and (token.pos_ in ['NOUN', 'VERB', 'ADJ']) and not token.is_punct:\n",
    "            new_text += token.text + ' '\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(POS_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  movie film like just really good story did bad movies',\n",
       " '  movie movies bad acting good did worst horrible think plot',\n",
       " '  episode series love family story man best life characters son',\n",
       " '  bad episode little like effects lake horror script monster pretty',\n",
       " '  really good liked great bourne thought story end action did',\n",
       " '  bad acting performance action character actors attempt plot davis role',\n",
       " '  story watch love series episode movies watching like worst acting',\n",
       " '  series bourne war just did gundam great episode really characters',\n",
       " '  did just character story think men got work know women',\n",
       " '  action scenes bourne just scene minutes films interesting time son']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',smooth_idf=True) # takes out other stops as well\n",
    "vectors = vectorizer.fit_transform(df['text'].to_list()).todense()\n",
    "vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "n_topics = 10\n",
    "svd = decomposition.TruncatedSVD(n_components=n_topics, algorithm='randomized')\n",
    "\n",
    "svd.fit(np.asarray(vectors))\n",
    "components = svd.components_\n",
    "print_topics(components, n_topics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to 5A\n",
    "\n",
    "You can see that there are clearer topics in my 5B topics than to my 5A topics. This is because we removed useless stop words, lemmatized and removed other distracting things like html tags and non ascii characters. \n",
    "\n",
    "The topics in 5A did not make much sense where as most of these topics follow one theme.\n",
    "- \"movie is bad\"\n",
    "- \"bad acting\"\n",
    "- \"bad series\"\n",
    "- \"horror\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c (5 points). Find 2 most similar pairs of reviews using document embeddings derived from SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "def cosine_similarity(v,u):\n",
    "    return (v @ u)/ (np.linalg.norm(v) * np.linalg.norm(u))\n",
    "\n",
    "def find_closest(V):\n",
    "    m = -np.inf\n",
    "    m1, m2 = -1, -1\n",
    "    for c1 in tqdm(range(1, V.shape[1])):\n",
    "        for c2 in range(1, V.shape[1]):\n",
    "            similarity = cosine_similarity(V[:,c1], V[:,c2])\n",
    "            if similarity > m and c1 != c2:\n",
    "                m = similarity\n",
    "                m1 = c1\n",
    "                m2 = c2\n",
    "    return m, m1, m2\n",
    "sim, i1, i2 = find_closest(components)\n",
    "print(sim, i1, i2)\n",
    "print(df_train.iloc[i1, 1])\n",
    "print(df_train.iloc[i2, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwnlo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af8207cabd4e670ce08470e4167faab7783bfd687a13b41d452e716ef3a3118e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
