{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "\n",
    "# import pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porblem 1 (25 ponits): Deep Learning Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: You can only use tensors for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(path/'training').ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(path/'testing').ls().sorted()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a (10 points). Create a 'typical' digit for each by averaging and classify the test data with them. Report your overall accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_distance(a, b): return (a - b).abs().mean((-1, -2))\n",
    "\n",
    "def is_digit(x, mean_digit, other_mean_digit): return mnist_distance(x,mean_digit) < mnist_distance(x,other_mean_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "digits = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "mean_representations = {}\n",
    "\n",
    "for digit in digits:\n",
    "    paths = (path/'training'/digit).ls().sorted()\n",
    "\n",
    "    digit_tensors = [tensor(Image.open(o)) for o in paths]\n",
    "\n",
    "    stacked_digits = torch.stack(digit_tensors).float() / 255\n",
    "\n",
    "    mean_rep = stacked_digits.mean(0)\n",
    "\n",
    "    mean_representations[digit] = mean_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "accs = []\n",
    "for digit in tqdm(digits):\n",
    "    paths = (path/'testing'/digit).ls().sorted()\n",
    "\n",
    "    digit_tensors = torch.stack([tensor(Image.open(o)) for o in paths])\n",
    "\n",
    "    mean_digit = mean_representations[digit]\n",
    "\n",
    "    final_preds = (torch.zeros(digit_tensors.shape[0], dtype=torch.bool) + 1).bool()\n",
    "    for other_digit in digits:\n",
    "\n",
    "        if other_digit == digit: continue\n",
    "\n",
    "        mean_other_digit = mean_representations[other_digit]\n",
    "\n",
    "        preds = is_digit(digit_tensors, mean_digit, mean_other_digit)\n",
    "\n",
    "        final_preds = torch.logical_and(preds, final_preds)\n",
    "    \n",
    "    acc = final_preds.float().mean()\n",
    "    accs.append(acc)\n",
    "    print(\"Accuracy for digit '{}':\\t{}\".format(digit,acc))\n",
    "print(\"Overall ACC: {}\".format(tensor(accs).float().mean()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b (10 points). Create a NN to classify digits (trained with regular SGD). Note that:\n",
    "- Since you deal with 10 categories here, it is preferrable to use cross entropy loss.\n",
    "- Again, anything other than tensors are not allowed in this problem so build your NN from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('training'),Path('testing')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.Grayscale(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.5], [0.5]), \n",
    "    transforms.Lambda(lambda x: torch.flatten(x)),\n",
    "    # transforms.Lambda(lambda x:F.one_hot(x,10)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = torchvision.datasets.ImageFolder((path/\"training\").as_posix(), transform = transform)\n",
    "\n",
    "# train_size = int(0.8 * len(full_dataset))\n",
    "# valid_size = 0\n",
    "train_size = len(full_dataset)\n",
    "valid_size = 0\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "testing_set = torchvision.datasets.ImageFolder((path/\"testing\").as_posix(), transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testing_set, batch_size=len(testing_set), shuffle=True)\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"test\": test_loader\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\tLoss: 3257.454345703125\n",
      "\tAccuracy: 0.09874733537435532\n",
      "Epoch: 1\n",
      "\tLoss: 2.3025851249694824\n",
      "\tAccuracy: 0.09868070483207703\n",
      "Epoch: 2\n",
      "\tLoss: 2.3025851249694824\n",
      "\tAccuracy: 0.0987306758761406\n",
      "Epoch: 3\n",
      "\tLoss: 2.3025851249694824\n",
      "\tAccuracy: 0.09871401637792587\n",
      "Epoch: 4\n",
      "\tLoss: 2.3025851249694824\n",
      "\tAccuracy: 0.0987306758761406\n",
      "Epoch: 5\n",
      "\tLoss: 2.3025851249694824\n",
      "\tAccuracy: 0.0987306758761406\n",
      "Epoch: 6\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6\n",
    "\n",
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
    "\n",
    "def linear(xb, w, b):\n",
    "    return xb@w + b\n",
    "\n",
    "def softmax(logits):\n",
    "    return torch.exp(logits + eps) / (torch.exp(logits + eps).sum(-1, keepdim=True) + eps)\n",
    "\n",
    "def log_prob(preds):\n",
    "    preds = preds.squeeze()\n",
    "    return -torch.log(preds + eps)\n",
    "\n",
    "def ce_loss(preds, yb):\n",
    "    # probs = softmax(preds)\n",
    "    # log_probs = log_prob(probs)\n",
    "    # return log_probs.mean()\n",
    "    return F.cross_entropy(preds, yb)\n",
    "\n",
    "def relu(x):\n",
    "    return x.clamp_min(0.)\n",
    "\n",
    "def accuracy(preds, yb):\n",
    "    preds = torch.argmax(softmax(preds), dim=1) == yb\n",
    "    return preds.float().mean()\n",
    "    # return correct.float().mean()\n",
    "\n",
    "def train(train_dl, epochs=10, lr=1e-2):\n",
    "    weights = init_params((28*28, 128))\n",
    "    bias = init_params(1)\n",
    "\n",
    "    h1_w = init_params((128, 64))\n",
    "    bias_h1 = init_params(1)    \n",
    "\n",
    "    out_w = init_params((64, 10))\n",
    "    out_b = init_params(1)\n",
    "\n",
    "    accs = []\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        per_epoch_loss = []\n",
    "        per_epoch_acc = []\n",
    "        for xb, yb in train_dl:\n",
    "            preds = relu(linear(xb, weights, bias))\n",
    "            preds = relu(linear(preds, h1_w, bias_h1))\n",
    "            preds = linear(preds, out_w, out_b)\n",
    "            # loss = ce_loss(preds, yb)\n",
    "            loss = ce_loss(preds, yb)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            per_epoch_loss.append(loss.item())\n",
    "            per_epoch_acc.append(accuracy(preds, yb).item())\n",
    "\n",
    "            weights.data -= weights.grad * lr\n",
    "            bias.data -= bias.grad * lr\n",
    "\n",
    "            h1_w.data -= h1_w.grad * lr\n",
    "            bias_h1.data -= bias_h1.grad * lr\n",
    "\n",
    "            out_w.data -= out_w.grad * lr\n",
    "            out_b.data -= out_b.grad * lr\n",
    "\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "\n",
    "            h1_w.grad.zero_()\n",
    "            bias_h1.grad.zero_()\n",
    "\n",
    "            out_w.grad.zero_()\n",
    "            out_b.grad.zero_()\n",
    "\n",
    "        accs.extend(per_epoch_acc)\n",
    "        losses.extend(per_epoch_loss)\n",
    "\n",
    "        print('\\tLoss: {}'.format(torch.mean(tensor(per_epoch_loss))))\n",
    "        print('\\tAccuracy: {}'.format(torch.mean(tensor(per_epoch_acc))))\n",
    "\n",
    "    \n",
    "    print('Overall Loss: {}'.format(torch.mean(tensor(losses))))\n",
    "    print('Overall Accuracy: {}'.format(torch.mean(tensor(accs))))\n",
    "        \n",
    "    return weights, bias\n",
    "\n",
    "def evaluate(test_dl, w, b):\n",
    "    # with torch.no_grad:\n",
    "    for xb, yb in test_dl:\n",
    "        preds = linear(xb, w, b)\n",
    "        acc = accuracy(preds, yb)\n",
    "        print(\"Test Accuracy: {}\".format(acc))\n",
    "\n",
    "w, b = train(train_loader, epochs=10, lr=1e-1)\n",
    "evaluate(test_loader, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_with_momentum(params, lr, mom):\n",
    "    for p in params:\n",
    "        p.data -= p.grad.data * lr + p.data * mom\n",
    "        p.grad.data.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c (5 points). Implement your own:\n",
    "- SGD with Momemtum\n",
    "- RMSProp\n",
    "\n",
    "by using two more tensors to tracked the smoothed gradients and L2 norms, respectively. Re-train your model on them and report performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD W/ Momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dl, epochs=10, lr=1e-2, mom=0.9):\n",
    "    weights = init_params((28*28, 128))\n",
    "    bias = init_params(1)\n",
    "    v_weights = 0\n",
    "    v_bias = 0\n",
    "\n",
    "    h1_w = init_params((128, 64))\n",
    "    bias_h1 = init_params(1)    \n",
    "    v_h1 = 0\n",
    "    v_bias_h1 = 0\n",
    "\n",
    "    out_w = init_params((64, 10))\n",
    "    out_b = init_params(1)\n",
    "    v_out_w = 0\n",
    "    v_out_b = 0\n",
    "\n",
    "    accs = []\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        per_epoch_loss = []\n",
    "        per_epoch_acc = []\n",
    "        for xb, yb in train_dl:\n",
    "            preds = relu(linear(xb, weights, bias))\n",
    "            preds = relu(linear(preds, h1_w, bias_h1))\n",
    "            preds = linear(preds, out_w, out_b)\n",
    "            # loss = ce_loss(preds, yb)\n",
    "            loss = ce_loss(preds, yb)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            per_epoch_loss.append(loss.item())\n",
    "            per_epoch_acc.append(accuracy(preds, yb).item())\n",
    "\n",
    "            mom_inv = (1-mom)\n",
    "            v_weights = mom * v_weights + mom_inv * weights.grad\n",
    "            v_bias = mom * v_bias + mom_inv * bias.grad\n",
    "\n",
    "            v_h1 = mom * v_h1 + h1_w.grad * mom_inv\n",
    "            v_bias_h1 = mom * v_bias_h1 + bias_h1.grad * mom_inv\n",
    "\n",
    "            v_out_w = mom * v_out_w + out_w.grad * mom_inv\n",
    "            v_out_b = mom * v_out_b + out_b.grad * mom_inv\n",
    "\n",
    "            weights.data -= v_weights * lr\n",
    "            bias.data -= v_bias * lr\n",
    "\n",
    "            h1_w.data -= v_h1 * lr\n",
    "            bias_h1.data -= v_bias_h1 * lr\n",
    "\n",
    "            out_w.data -= v_out_w * lr\n",
    "            out_b.data -= v_out_b * lr\n",
    "\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "\n",
    "            h1_w.grad.zero_()\n",
    "            bias_h1.grad.zero_()\n",
    "\n",
    "            out_w.grad.zero_()\n",
    "            out_b.grad.zero_()\n",
    "\n",
    "        accs.extend(per_epoch_acc)\n",
    "        losses.extend(per_epoch_loss)\n",
    "\n",
    "        print('\\tLoss: {}'.format(torch.mean(tensor(per_epoch_loss))))\n",
    "        print('\\tAccuracy: {}'.format(torch.mean(tensor(per_epoch_acc))))\n",
    "\n",
    "    \n",
    "    print('Overall Loss: {}'.format(torch.mean(tensor(losses))))\n",
    "    print('Overall Accuracy: {}'.format(torch.mean(tensor(accs))))\n",
    "        \n",
    "    return weights, bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp\n",
    "\n",
    "Code selected from NN class code above:\n",
    "\n",
    "```\n",
    "def fit_rms_prop(self, dls, decay_rate=0.9):\n",
    "        cache_input = torch.zeros_like(self.weights_input_hidden)\n",
    "        cache_output = torch.zeros_like(self.weights_hidden_output)\n",
    "\n",
    "        overall_loss = []\n",
    "        for i in range(self.epochs):\n",
    "            losses = []\n",
    "            print('Starting epoch: {}'.format(i))\n",
    "            for batch in dls.train:\n",
    "                data, labels = batch[0], batch[1]\n",
    "                data = tensor(data, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "                labels = tensor(labels, dtype=torch.uint8, device=self.device,  requires_grad=True)\n",
    "\n",
    "                data = data.to(torch.float64)\n",
    "                labels = labels.to(torch.uint8)\n",
    "                x = self.forward(data)\n",
    "\n",
    "                loss = self.cross_entropy_loss(x, labels)\n",
    "                # self.loss(x, labels)\n",
    "                losses.append(loss.detach().float())\n",
    "                overall_loss.append(loss.detach().float())\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    cache_input = decay_rate * cache_input + (1 - decay_rate) * self.weights_input_hidden.grad\n",
    "                    cache_output = decay_rate * cache_output + (1 - decay_rate) * self.weights_hidden_output.grad\n",
    "\n",
    "                    self.weights_input_hidden -= (self.lr * self.weights_input_hidden.grad) / (torch.sqrt(cache_input) + self.eps)\n",
    "                    self.weights_hidden_output -= (self.lr * self.weights_hidden_output.grad) / (torch.sqrt(cache_output) + self.eps)\n",
    "\n",
    "            print('\\t loss = {}'.format(tensor(loss).float().mean()))\n",
    "        print('\\t overall loss = {}'.format(tensor(overall_loss).float().mean()))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (25 points): Getting Your Data Ready For a NN Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('train.txt'),Path('valid.txt')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eight thousand one . eight thousand two . eight thousand three . eight thousand four . eight thousand five . eight thousand six . eight thousand seven . eight thousand eight . eight thousand nine . eight thousand ten . eight thousand eleven . eight thousand twelve . eight thousand thirteen . eight thousand fourteen . eight thousand fifteen . eight thousand sixteen . eight thousand seventeen . eight thousand eighteen . eight thousand nineteen . eight thousand twenty . eight thousand twenty one . eight thousand twenty two . eight thousand twenty three . eight thousand twenty four . eight thousand twenty five . eight thousand twenty six . eight thousand twenty seven . eight thousand twenty eight . eight thousand twenty nine . eight thousand thirty . eight thousand thirty one . eight thousand thirty two . eight thousand thirty three . eight thousand thirty four . eight thousand thirty five . eight thousand thirty six . eight thousand thirty seven . eight thousand thirty eight . eight thous'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [line for p in path.ls()[::-1] for line in p.read_text().split(' \\n')]\n",
    "text = ' . '.join([l for l in lines])\n",
    "text[:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a (5 points). Split **text** by space to get **tokens** and create a **vocab** with unique **tokens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_space(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "def create_vocab_from_tokens(tokens):\n",
    "    # vocab = {token for i, token in enumerate(set(tokens))}\n",
    "    # return list(vocab)\n",
    "    return L(*tokens).unique()\n",
    "\n",
    "tokens = tokenize_by_space(text)\n",
    "vocab = create_vocab_from_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#31) ['eight','thousand','one','.','two','three','four','five','six','seven'...]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b (5 points). Create a mapping from word to indices and convert **tokens** into **nums**, which is a list of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2idx(vocab):\n",
    "    return {token: idx for idx, token in enumerate(vocab)}\n",
    "\n",
    "def convert_tokens_to_ids(tokens, word2idx):\n",
    "    return L(word2idx[i] for i in tokens)\n",
    "\n",
    "word2idx = create_word2idx(vocab)\n",
    "nums = convert_tokens_to_ids(tokens, word2idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c (5 points). Create a _dataset_ where each tuple is the input and output of a NN language model. The sequence length of inputs and outputs should be 32. Then create a _training set_ with the first 80% data and a _validation set_ with the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = 32\n",
    "\n",
    "def create_tuples(nums):\n",
    "    return L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
    "         for i in range(0,len(nums)-sl-1,sl))\n",
    "\n",
    "def train_test_split(sequences):\n",
    "    train_size = int(len(sequences) * 0.8)\n",
    "    train, test = sequences[:train_size], sequences[train_size:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = create_tuples(nums)\n",
    "# dset\n",
    "train_ds, valid_ds = train_test_split(dset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d (10 points). Reorder your training set and validation set to be ready for _dataloaders_ to be used in a NN language model. Note that:\n",
    "- `m = len(dset) // 64` batches will be created in the new order without shuffling (the first 64 rows will be the first batch, the next 64 rows will be the second batch, and so on).\n",
    "- The new first 64 rows should be the `1st, (1+m)-th, (1+2m)-th, ..., (1+64m)-th` rows in the original corresponding dataset; The next 64 rows should be the `2nd, (2+m)-th, (2+2m)-th, ..., (2+64m)-th` rows; and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reorder_dataset(ds, bs):\n",
    "    m = len(ds) // bs\n",
    "    new_ds = L()\n",
    "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n",
    "    return new_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_reordered = reorder_dataset(train_ds, bs=64)\n",
    "valid_ds_reordered = reorder_dataset(valid_ds, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dsets(\n",
    "    train_ds_reordered,\n",
    "    valid_ds_reordered,\n",
    "    bs=64, drop_last=True, shuffle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (25 points): Simple NN Language Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a (15 points). The stacked/unrolled representation depict the same 2-layer RNN. Implement this RNN with only:\n",
    "- torch.tensor\n",
    "- tensor functions (from torch.tensor or torch)\n",
    "- torch.nn.Embedding\n",
    "- torch.nn.Linear\n",
    "- torch.nn.relu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"2-layer RNN\" width=\"550\" caption=\"2-layer RNN\" src=\"./att_00025.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"2-layer unrolled RNN\" width=\"550\" caption=\"Two-layer unrolled RNN\" src=\"./att_00026.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACoolRNN(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)   \n",
    "        self.h_h_2 = nn.Linear(n_hidden, n_hidden)  \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "        self.h = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for i in range(sl):\n",
    "            self.h = self.h + self.i_h(x[:,i])\n",
    "            self.h = F.relu(self.h_h(self.h))\n",
    "            self.h2 = F.relu(self.h_h_2(self.h)) + self.h\n",
    "            outs.append(self.h_o(self.h))\n",
    "        self.h = self.h.detach()\n",
    "        return torch.stack(outs, dim=1)\n",
    "    \n",
    "    def reset(self): self.h = 0\n",
    "\n",
    "def loss_func(inp, targ):\n",
    "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b (10 points). Find the best learning rate and train your NN with 1cycle policy and Adam (using _Learner_ class from fastai). Report your result with a reasonable metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.290276</td>\n",
       "      <td>3.195672</td>\n",
       "      <td>0.206217</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.664793</td>\n",
       "      <td>1.972941</td>\n",
       "      <td>0.470215</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.093985</td>\n",
       "      <td>1.853706</td>\n",
       "      <td>0.467936</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.780326</td>\n",
       "      <td>1.837062</td>\n",
       "      <td>0.475505</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.584092</td>\n",
       "      <td>1.764385</td>\n",
       "      <td>0.498942</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.445986</td>\n",
       "      <td>1.736075</td>\n",
       "      <td>0.522542</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.347024</td>\n",
       "      <td>1.806055</td>\n",
       "      <td>0.532715</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.272966</td>\n",
       "      <td>1.828314</td>\n",
       "      <td>0.534424</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.218623</td>\n",
       "      <td>1.819970</td>\n",
       "      <td>0.540365</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.181584</td>\n",
       "      <td>1.822541</td>\n",
       "      <td>0.538981</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the best learning rate and train your NN with 1cycle policy and Adam (using Learner class from fastai). Report your result with a reasonable metric\n",
    "\n",
    "learner = Learner(dls, ACoolRNN(len(vocab), 64), loss_func=loss_func, metrics=accuracy, opt_func=Adam, cbs=ModelResetter)\n",
    "# learner = Learner(dls, TwoLayerRNN(seq_len, 512, seq_len), loss_func=nn.CrossEntropyLoss(), metrics=accuracy, opt_func=Adam)\n",
    "learner.fit_one_cycle(10, 3e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (25 points):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a (10 points). Reuse the IMDB reviews dataset from Homework 1. Tokenize (experiment with the vocab size yourself) it with the subword tokenizer you developed in Homework 1 and create _Dataloaders_ in the similar way of Problem 2 to get ready for a LSTM Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b (10 points). Implement a 2-layer LSTM Language Model that has:\n",
    "- One dropout layer (with 50% chance)\n",
    "- Weight tying between the input and output embedding layers\n",
    "- Activation regularization\n",
    "- Temporal activation regularization\n",
    "\n",
    "You can use anything, such as the LSTM module from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers, p_dropout, bs):\n",
    "\n",
    "    def reset(self): \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNRegularizer(Callback):\n",
    "    def __init__(self, alpha=0., beta=0.): self.alpha, self.beta = alpha, beta\n",
    "\n",
    "    # This is called after the forward pass of the model\n",
    "    def after_pred(self):\n",
    "        # Store inside this class the raw output of the model and the dropped out output, respectively \n",
    "        self.raw, self.out = \n",
    "        # Modify the model's output to be just the activation of the last layer\n",
    "        self.learn.pred = \n",
    "\n",
    "    # This is called after the normal loss is computed\n",
    "    def after_loss(self):\n",
    "        if not self.training: return\n",
    "        if self.alpha != 0.:\n",
    "            self.learn.loss += \n",
    "        if self.beta != 0.:\n",
    "            self.learn.loss += "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c (5 points). Find the best learning rate and train your NN with 1cycle policy and Adam. Report your result with a reasonable metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, \n",
    "                # ...\n",
    "                cbs=[ModelResetter, MyRNNRegularizer(alpha=2, beta=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwnlo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af8207cabd4e670ce08470e4167faab7783bfd687a13b41d452e716ef3a3118e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
