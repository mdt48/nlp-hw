{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "\n",
    "# import pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porblem 1 (25 ponits): Deep Learning Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: You can only use tensors for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(path/'training').ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(path/'testing').ls().sorted()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a (10 points). Create a 'typical' digit for each by averaging and classify the test data with them. Report your overall accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_distance(a, b): return (a - b).abs().mean((-1, -2))\n",
    "\n",
    "def is_digit(x, mean_digit, other_mean_digit): return mnist_distance(x,mean_digit) < mnist_distance(x,other_mean_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "digits = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "mean_representations = {}\n",
    "\n",
    "for digit in digits:\n",
    "    paths = (path/'training'/digit).ls().sorted()\n",
    "\n",
    "    digit_tensors = [tensor(Image.open(o)) for o in paths]\n",
    "\n",
    "    stacked_digits = torch.stack(digit_tensors).float() / 255\n",
    "\n",
    "    mean_rep = stacked_digits.mean(0)\n",
    "\n",
    "    mean_representations[digit] = mean_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "accs = []\n",
    "for digit in tqdm(digits):\n",
    "    paths = (path/'testing'/digit).ls().sorted()\n",
    "\n",
    "    digit_tensors = torch.stack([tensor(Image.open(o)) for o in paths])\n",
    "\n",
    "    mean_digit = mean_representations[digit]\n",
    "\n",
    "    final_preds = (torch.zeros(digit_tensors.shape[0], dtype=torch.bool) + 1).bool()\n",
    "    for other_digit in digits:\n",
    "\n",
    "        if other_digit == digit: continue\n",
    "\n",
    "        mean_other_digit = mean_representations[other_digit]\n",
    "\n",
    "        preds = is_digit(digit_tensors, mean_digit, mean_other_digit)\n",
    "\n",
    "        final_preds = torch.logical_and(preds, final_preds)\n",
    "    \n",
    "    acc = final_preds.float().mean()\n",
    "    accs.append(acc)\n",
    "    print(\"Accuracy for digit '{}':\\t{}\".format(digit,acc))\n",
    "print(\"Overall ACC: {}\".format(tensor(accs).float().mean()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b (10 points). Create a NN to classify digits (trained with regular SGD). Note that:\n",
    "- Since you deal with 10 categories here, it is preferrable to use cross entropy loss.\n",
    "- Again, anything other than tensors are not allowed in this problem so build your NN from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Transform):\n",
    "    def encodes(self, x: PIL.Image.Image):\n",
    "        return tensor(np.array(x).flatten())\n",
    "\n",
    "# Create the DataBlock with the Flatten transform\n",
    "datablock = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),\n",
    "                      get_items=get_image_files,\n",
    "                      splitter=RandomSplitter(),\n",
    "                      get_y=parent_label,\n",
    "                      item_tfms=[Resize(28, method=ResizeMethod.Squish, resamples=(1, 1)), \n",
    "                                 Flatten(),\n",
    "                                 ToTensor(),\n",
    "                                    Normalize.from_stats(*imagenet_stats)\n",
    "                                ])\n",
    "\n",
    "# Create the DataLoaders with batch size of 64\n",
    "dls = datablock.dataloaders(path, bs=64)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "dls.to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN:\n",
    "    def __init__(self, layers, lr, epochs, optim='sgd', gamma=0.9) -> None:\n",
    "        # self.input_size = input_size\n",
    "        # self.hidden_size = hidden_size\n",
    "        # self.output_size = output_size\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs    \n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        self.soft_max = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        self.eps = 1e-6\n",
    "\n",
    "        if optim == 'sgd':\n",
    "            self.opt = self.sgd\n",
    "        elif optim == 'sgd_mom':\n",
    "            self.opt = self.sgd_momentum\n",
    "        elif optim == 'rms':\n",
    "            self.opt = self.rms_prop\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create random weight tensors for input to hidden and hidden to output layers\n",
    "        # self.weights_input_hidden = torch.randn(input_size, hidden_size, requires_grad=True, device=self.device, dtype=torch.float64)\n",
    "        # self.weights_hidden_output = torch.randn(hidden_size, output_size, requires_grad=True, device=self.device, dtype=torch.float64)\n",
    "\n",
    "        self.weights = {}\n",
    "        self.n_layers = len(layers)\n",
    "        for i in range(len(layers)):\n",
    "            self.weights['w'+str(i)] = torch.randn(layers[i][0], layers[i][1], requires_grad=True, device=self.device, dtype=torch.float64) \n",
    "\n",
    "            if optim == 'sgd_mom' or optim == 'rms':\n",
    "                self.weights['v'+str(i)] = torch.zeros_like(self.weights['w'+str(i)])\n",
    "\n",
    "    def train(self, dls):\n",
    "        for i in range(self.epochs):\n",
    "            losses = []\n",
    "            accs = []\n",
    "            print('Starting epoch: {}'.format(i))\n",
    "            for batch in dls.train:\n",
    "                data, labels = batch[0], batch[1]\n",
    "                data = tensor(data, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "                labels = tensor(labels, dtype=torch.uint8, device=self.device,  requires_grad=True)\n",
    "\n",
    "                data = data.to(torch.float64)\n",
    "                labels = labels.to(torch.uint8)\n",
    "                x = self.forward(data)\n",
    "\n",
    "                loss = self.cross_entropy_loss(x, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "\n",
    "                losses.append(loss.detach().float())\n",
    "\n",
    "\n",
    "                accs.append(self.accuracy(x, labels))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    self.opt()\n",
    "\n",
    "            print('\\t loss = {}'.format(tensor(loss).float().mean()))\n",
    "            print('\\t acc = {}'.format(tensor(accs).float().mean()))\n",
    "\n",
    "    def accuracy(self, preds, labels):\n",
    "        \"\"\"sumary_line\n",
    "        \n",
    "        Keyword arguments:\n",
    "        labels -- list of integer labels\n",
    "        preds -- list of float probabilities for each label per data point\n",
    "        Return: accuract of preds\n",
    "        \"\"\"\n",
    "        # s = self.soft_max(preds)\n",
    "        s = self.soft_max(preds)\n",
    "        preds = torch.argmax(s, dim=1)\n",
    "        return (preds == labels).float().mean()\n",
    "    \n",
    "    def eval(self, dls):\n",
    "        accs = []\n",
    "        with torch.no_grad:\n",
    "            for batch in dls.test:\n",
    "                data, labels = batch[0], batch[1]\n",
    "                data = tensor(data, dtype=torch.float32, device=self.device)\n",
    "                labels = tensor(labels, dtype=torch.uint8, device=self.device)\n",
    "\n",
    "                data = data.to(torch.float64)\n",
    "                labels = labels.to(torch.uint8)\n",
    "                x = self.forward(data)\n",
    "\n",
    "                preds = torch.argmax(x, dim=1)\n",
    "                acc = (preds == labels).float().mean()\n",
    "                accs.append(acc)\n",
    "        print('Accuracy: {}'.format(tensor(accs).float().mean()))\n",
    "\n",
    "    def sgd(self):\n",
    "        for i in range(self.n_layers):\n",
    "            self.weights['w'+str(i)] -= self.lr * self.weights['w'+str(i)].grad\n",
    "            # self.weights['b'+str(i)] -= self.lr * self.weights['b'+str(i)].grad\n",
    "\n",
    "    def sgd_momentum(self):\n",
    "        for i in range(self.n_layers):\n",
    "            self.weights['v'+str(i)] = self.gamma * self.weights['v'+str(i)] - self.lr * self.weights['w'+str(i)]\n",
    "            self.weights['w'+str(i)] += self.weights['v'+str(i)]\n",
    "            \n",
    "    def rms_prop(self):\n",
    "        for i in range(self.n_layers):\n",
    "            self.weights['v'+str(i)] = self.gamma * self.weights['v'+str(i)] + (1 - self.gamma) * self.weights['w'+str(i)].grad**2\n",
    "            self.weights['w'+str(i)] += (self.lr * self.weights['w'+str(i)].grad) / (torch.sqrt(self.weights['v'+str(i)]+ self.eps) )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = torch.matmul(batch, self.weights['w'+str(0)])\n",
    "        # x = self.sigmoid_activation(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # print(self.weights['w'+str(0)].is_leaf)\n",
    "        for i in range(1, self.n_layers):\n",
    "            x = torch.matmul(x, self.weights['w'+str(i)])\n",
    "            if i != self.n_layers-1:\n",
    "                # x = self.sigmoid_activation(x)\n",
    "                x = self.activation(x)\n",
    "        # print(self.weights['w'+str(i)].is_leaf)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def sigmoid_activation(self, x):\n",
    "        return 1 / (1 + torch.exp(-x + self.eps))\n",
    "    \n",
    "    def cross_entropy_loss(self, predictions, targets):\n",
    "        batch_size = predictions.size(0)\n",
    "        # s = self.soft_max(predictions)\n",
    "        s = self.soft_max(predictions)\n",
    "\n",
    "        log_liklihood = -torch.log(s[range(batch_size), targets.tolist()] + self.eps)\n",
    "        loss = log_liklihood.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m MyNN([(\u001b[39m784\u001b[39m, \u001b[39m5\u001b[39m), (\u001b[39m5\u001b[39m, \u001b[39m10\u001b[39m)], lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mtrain(dls)\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39meval(dls)\n",
      "Cell \u001b[0;32mIn[45], line 47\u001b[0m, in \u001b[0;36mMyNN.train\u001b[0;34m(self, dls)\u001b[0m\n\u001b[1;32m     45\u001b[0m accs \u001b[39m=\u001b[39m []\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStarting epoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i))\n\u001b[0;32m---> 47\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dls\u001b[39m.\u001b[39mtrain:\n\u001b[1;32m     48\u001b[0m     data, labels \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m], batch[\u001b[39m1\u001b[39m]\n\u001b[1;32m     49\u001b[0m     data \u001b[39m=\u001b[39m tensor(data, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/data/load.py:127\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbefore_iter()\n\u001b[1;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__idxs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_idxs() \u001b[39m# called in context of main process (not workers/subprocesses)\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m _loaders[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_l\u001b[39m.\u001b[39mnum_workers\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_l):\n\u001b[1;32m    128\u001b[0m     \u001b[39m# pin_memory causes tuples to be converted to lists, so convert them back to tuples\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpin_memory \u001b[39mand\u001b[39;00m \u001b[39mtype\u001b[39m(b) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m: b \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(b)\n\u001b[1;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: b \u001b[39m=\u001b[39m to_device(b, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:39\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_iter)\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/data/load.py:138\u001b[0m, in \u001b[0;36mDataLoader.create_batches\u001b[0;34m(self, samps)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mit \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset)\n\u001b[1;32m    137\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m o:o \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_item, samps))\n\u001b[0;32m--> 138\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunkify(res))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastcore/basics.py:230\u001b[0m, in \u001b[0;36mchunked\u001b[0;34m(it, chunk_sz, drop_last, n_chunks)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(it, Iterator): it \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(it)\n\u001b[1;32m    229\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(it, chunk_sz))\n\u001b[1;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mand\u001b[39;00m (\u001b[39mlen\u001b[39m(res)\u001b[39m==\u001b[39mchunk_sz \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m drop_last): \u001b[39myield\u001b[39;00m res\n\u001b[1;32m    232\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(res)\u001b[39m<\u001b[39mchunk_sz: \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/data/load.py:153\u001b[0m, in \u001b[0;36mDataLoader.do_item\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_item\u001b[39m(\u001b[39mself\u001b[39m, s):\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter_item(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_item(s))\n\u001b[1;32m    154\u001b[0m     \u001b[39mexcept\u001b[39;00m SkipItemException: \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/data/load.py:160\u001b[0m, in \u001b[0;36mDataLoader.create_item\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_item\u001b[39m(\u001b[39mself\u001b[39m, s):\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindexed: \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[s \u001b[39mor\u001b[39;49;00m \u001b[39m0\u001b[39;49m]\n\u001b[1;32m    161\u001b[0m     \u001b[39melif\u001b[39;00m s \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mit)\n\u001b[1;32m    162\u001b[0m     \u001b[39melse\u001b[39;00m: \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index an iterable dataset numerically - must use `None`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/data/core.py:455\u001b[0m, in \u001b[0;36mDatasets.__getitem__\u001b[0;34m(self, it)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, it):\n\u001b[0;32m--> 455\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([tl[it] \u001b[39mfor\u001b[39;00m tl \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls])\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m res \u001b[39mif\u001b[39;00m is_indexer(it) \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mres))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/data/core.py:455\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, it):\n\u001b[0;32m--> 455\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([tl[it] \u001b[39mfor\u001b[39;00m tl \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls])\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m res \u001b[39mif\u001b[39;00m is_indexer(it) \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mres))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/data/core.py:414\u001b[0m, in \u001b[0;36mTfmdLists.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    412\u001b[0m res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(idx)\n\u001b[1;32m    413\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_item \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_after_item(res) \u001b[39mif\u001b[39;00m is_indexer(idx) \u001b[39melse\u001b[39;00m res\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_item)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/data/core.py:374\u001b[0m, in \u001b[0;36mTfmdLists._after_item\u001b[0;34m(self, o)\u001b[0m\n\u001b[0;32m--> 374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_after_item\u001b[39m(\u001b[39mself\u001b[39m, o): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtfms(o)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastcore/transform.py:208\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, o)\u001b[0m\n\u001b[0;32m--> 208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, o): \u001b[39mreturn\u001b[39;00m compose_tfms(o, tfms\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs, split_idx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit_idx)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastcore/transform.py:158\u001b[0m, in \u001b[0;36mcompose_tfms\u001b[0;34m(x, tfms, is_enc, reverse, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m tfms:\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_enc: f \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mdecode\n\u001b[0;32m--> 158\u001b[0m     x \u001b[39m=\u001b[39m f(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastcore/transform.py:81\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m---> 81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m'\u001b[39;49m\u001b[39mencodes\u001b[39;49m\u001b[39m'\u001b[39;49m, x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastcore/transform.py:91\u001b[0m, in \u001b[0;36mTransform._call\u001b[0;34m(self, fn, x, split_idx, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, fn, x, split_idx\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m split_idx\u001b[39m!=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, fn), x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastcore/transform.py:97\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[0;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     96\u001b[0m     ret \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreturns(x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f,\u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mreturn\u001b[39;00m retain_type(f(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), x, ret)\n\u001b[1;32m     98\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(f, x_, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastcore/dispatch.py:120\u001b[0m, in \u001b[0;36mTypeDispatch.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minst \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: f \u001b[39m=\u001b[39m MethodType(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minst)\n\u001b[1;32m    119\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mowner \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: f \u001b[39m=\u001b[39m MethodType(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mowner)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/vision/core.py:123\u001b[0m, in \u001b[0;36mPILBase.create\u001b[0;34m(cls, fn, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fn,ndarray): \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(Image\u001b[39m.\u001b[39mfromarray(fn))\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fn,\u001b[39mbytes\u001b[39m): fn \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(fn)\n\u001b[0;32m--> 123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(load_image(fn, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmerge(\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_open_args, kwargs)))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/vision/core.py:98\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(fn, mode)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_image\u001b[39m(fn, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     97\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mOpen and load a `PIL.Image` and convert to `mode`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 98\u001b[0m     im \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(fn)\n\u001b[1;32m     99\u001b[0m     im\u001b[39m.\u001b[39mload()\n\u001b[1;32m    100\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39m_new(im\u001b[39m.\u001b[39mim)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/PIL/Image.py:3126\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3124\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fp, Path):\n\u001b[0;32m-> 3126\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(fp\u001b[39m.\u001b[39;49mresolve())\n\u001b[1;32m   3127\u001b[0m \u001b[39melif\u001b[39;00m is_path(fp):\n\u001b[1;32m   3128\u001b[0m     filename \u001b[39m=\u001b[39m fp\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/pathlib.py:1077\u001b[0m, in \u001b[0;36mPath.resolve\u001b[0;34m(self, strict)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSymlink loop from \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\u001b[39m.\u001b[39mfilename)\n\u001b[1;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1077\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mrealpath(\u001b[39mself\u001b[39;49m, strict\u001b[39m=\u001b[39;49mstrict)\n\u001b[1;32m   1078\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1079\u001b[0m     check_eloop(e)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/posixpath.py:395\u001b[0m, in \u001b[0;36mrealpath\u001b[0;34m(filename, strict)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[39m\"\"\"Return the canonical path of the specified filename, eliminating any\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[39msymbolic links encountered in the path.\"\"\"\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(filename)\n\u001b[0;32m--> 395\u001b[0m     path, ok \u001b[39m=\u001b[39m _joinrealpath(filename[:\u001b[39m0\u001b[39;49m], filename, strict, {})\n\u001b[1;32m    396\u001b[0m     \u001b[39mreturn\u001b[39;00m abspath(path)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/posixpath.py:428\u001b[0m, in \u001b[0;36m_joinrealpath\u001b[0;34m(path, rest, strict, seen)\u001b[0m\n\u001b[1;32m    426\u001b[0m         path \u001b[39m=\u001b[39m pardir\n\u001b[1;32m    427\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m newpath \u001b[39m=\u001b[39m join(path, name)\n\u001b[1;32m    429\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlstat(newpath)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/posixpath.py:71\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39mreturn\u001b[39;00m s\u001b[39m.\u001b[39mstartswith(sep)\n\u001b[1;32m     67\u001b[0m \u001b[39m# Join pathnames.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m# Ignore the previous parts if a part is absolute.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m# Insert a '/' unless the first part is empty or already ends in '/'.\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(a, \u001b[39m*\u001b[39mp):\n\u001b[1;32m     72\u001b[0m     \u001b[39m\"\"\"Join two or more pathname components, inserting '/' as needed.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m    If any component is an absolute path, all previous path components\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m    will be discarded.  An empty last part will result in a path that\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m    ends with a separator.\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     a \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(a)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "model = MyNN([(784, 5), (5, 10)], lr=0.01, epochs=10)\n",
    "model.train(dls)\n",
    "model.eval(dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNN([784, 256, 128, 64, 10], lr=0.01, epochs=10, optim='sgd_mom')\n",
    "model.train(dls)\n",
    "model.eval(dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNN([784, 256, 128, 64, 10], lr=0.01, epochs=30, optim='rms')\n",
    "model.train(dls)\n",
    "model.eval(dls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cant use torch ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 7 * 7, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, opt_func=SGD, loss_func=nn.CrossEntropyLoss(), metrics=accuracy)\n",
    "learn.fit(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c (5 points). Implement your own:\n",
    "- SGD with Momemtum\n",
    "- RMSProp\n",
    "\n",
    "by using two more tensors to tracked the smoothed gradients and L2 norms, respectively. Re-train your model on them and report performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD W/ Momentum \n",
    "\n",
    "Code selected from NN class code above:\n",
    "\n",
    "```\n",
    "\n",
    "def fit_sgd_momentum(self, dls, momentum=0.9):\n",
    "        velocity_input = torch.zeros_like(self.weights_input_hidden)\n",
    "        velocity_output = torch.zeros_like(self.weights_hidden_output)\n",
    "\n",
    "        overall_loss = []\n",
    "        for i in range(self.epochs):\n",
    "            losses = []\n",
    "            print('Starting epoch: {}'.format(i))\n",
    "            for batch in dls.train:\n",
    "                data, labels = batch[0], batch[1]\n",
    "                data = tensor(data, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "                labels = tensor(labels, dtype=torch.uint8, device=self.device,  requires_grad=True)\n",
    "\n",
    "                data = data.to(torch.float64)\n",
    "                labels = labels.to(torch.uint8)\n",
    "                x = self.forward(data)\n",
    "\n",
    "                loss = self.cross_entropy_loss(x, labels)\n",
    "                # self.loss(x, labels)\n",
    "                losses.append(loss.detach().float())\n",
    "                overall_loss.append(loss.detach().float())\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    velocity_input = momentum * velocity_input - self.lr * self.weights_input_hidden.grad\n",
    "                    velocity_output = momentum * velocity_output - self.lr * self.weights_hidden_output.grad\n",
    "\n",
    "                    self.weights_input_hidden += velocity_input\n",
    "                    self.weights_hidden_output += velocity_output\n",
    "\n",
    "            print('\\t loss = {}'.format(tensor(loss).float().mean()))\n",
    "        print('\\t overall loss = {}'.format(tensor(overall_loss).float().mean()))\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp\n",
    "\n",
    "Code selected from NN class code above:\n",
    "\n",
    "```\n",
    "def fit_rms_prop(self, dls, decay_rate=0.9):\n",
    "        cache_input = torch.zeros_like(self.weights_input_hidden)\n",
    "        cache_output = torch.zeros_like(self.weights_hidden_output)\n",
    "\n",
    "        overall_loss = []\n",
    "        for i in range(self.epochs):\n",
    "            losses = []\n",
    "            print('Starting epoch: {}'.format(i))\n",
    "            for batch in dls.train:\n",
    "                data, labels = batch[0], batch[1]\n",
    "                data = tensor(data, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "                labels = tensor(labels, dtype=torch.uint8, device=self.device,  requires_grad=True)\n",
    "\n",
    "                data = data.to(torch.float64)\n",
    "                labels = labels.to(torch.uint8)\n",
    "                x = self.forward(data)\n",
    "\n",
    "                loss = self.cross_entropy_loss(x, labels)\n",
    "                # self.loss(x, labels)\n",
    "                losses.append(loss.detach().float())\n",
    "                overall_loss.append(loss.detach().float())\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    cache_input = decay_rate * cache_input + (1 - decay_rate) * self.weights_input_hidden.grad\n",
    "                    cache_output = decay_rate * cache_output + (1 - decay_rate) * self.weights_hidden_output.grad\n",
    "\n",
    "                    self.weights_input_hidden -= (self.lr * self.weights_input_hidden.grad) / (torch.sqrt(cache_input) + self.eps)\n",
    "                    self.weights_hidden_output -= (self.lr * self.weights_hidden_output.grad) / (torch.sqrt(cache_output) + self.eps)\n",
    "\n",
    "            print('\\t loss = {}'.format(tensor(loss).float().mean()))\n",
    "        print('\\t overall loss = {}'.format(tensor(overall_loss).float().mean()))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (25 points): Getting Your Data Ready For a NN Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('train.txt'),Path('valid.txt')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eight thousand one . eight thousand two . eight thousand three . eight thousand four . eight thousand five . eight thousand six . eight thousand seven . eight thousand eight . eight thousand nine . eight thousand ten . eight thousand eleven . eight thousand twelve . eight thousand thirteen . eight thousand fourteen . eight thousand fifteen . eight thousand sixteen . eight thousand seventeen . eight thousand eighteen . eight thousand nineteen . eight thousand twenty . eight thousand twenty one . eight thousand twenty two . eight thousand twenty three . eight thousand twenty four . eight thousand twenty five . eight thousand twenty six . eight thousand twenty seven . eight thousand twenty eight . eight thousand twenty nine . eight thousand thirty . eight thousand thirty one . eight thousand thirty two . eight thousand thirty three . eight thousand thirty four . eight thousand thirty five . eight thousand thirty six . eight thousand thirty seven . eight thousand thirty eight . eight thous'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [line for p in path.ls()[::-1] for line in p.read_text().split(' \\n')]\n",
    "text = ' . '.join([l for l in lines])\n",
    "text[:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a (5 points). Split **text** by space to get **tokens** and create a **vocab** with unique **tokens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_space(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "def create_vocab_from_tokens(tokens):\n",
    "    # vocab = {token for i, token in enumerate(set(tokens))}\n",
    "    # return list(vocab)\n",
    "    return L(*tokens).unique()\n",
    "\n",
    "tokens = tokenize_by_space(text)\n",
    "vocab = create_vocab_from_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#31) ['eight','thousand','one','.','two','three','four','five','six','seven'...]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b (5 points). Create a mapping from word to indices and convert **tokens** into **nums**, which is a list of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2idx(vocab):\n",
    "    return {token: idx for idx, token in enumerate(vocab)}\n",
    "\n",
    "def convert_tokens_to_ids(tokens, word2idx):\n",
    "    return L(word2idx[i] for i in tokens)\n",
    "\n",
    "word2idx = create_word2idx(vocab)\n",
    "nums = convert_tokens_to_ids(tokens, word2idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c (5 points). Create a _dataset_ where each tuple is the input and output of a NN language model. The sequence length of inputs and outputs should be 32. Then create a _training set_ with the first 80% data and a _validation set_ with the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = 32\n",
    "\n",
    "def create_tuples(nums):\n",
    "    return L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
    "         for i in range(0,len(nums)-sl-1,sl))\n",
    "\n",
    "def train_test_split(sequences):\n",
    "    train_size = int(len(sequences) * 0.8)\n",
    "    train, test = sequences[:train_size], sequences[train_size:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = create_tuples(nums)\n",
    "# dset\n",
    "train_ds, valid_ds = train_test_split(dset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d (10 points). Reorder your training set and validation set to be ready for _dataloaders_ to be used in a NN language model. Note that:\n",
    "- `m = len(dset) // 64` batches will be created in the new order without shuffling (the first 64 rows will be the first batch, the next 64 rows will be the second batch, and so on).\n",
    "- The new first 64 rows should be the `1st, (1+m)-th, (1+2m)-th, ..., (1+64m)-th` rows in the original corresponding dataset; The next 64 rows should be the `2nd, (2+m)-th, (2+2m)-th, ..., (2+64m)-th` rows; and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_dataset(dataset):\n",
    "    m = len(dataset) // 64\n",
    "    new_dset = []\n",
    "    for i in range(64):\n",
    "        for j in range(m):\n",
    "            t = dataset[i+j*64]\n",
    "            new_dset.append(t)\n",
    "            # new_dset.append(dataset[i+j*64])\n",
    "    # print(torch.stack(new_dset).shape)\n",
    "    # print(torch.tensor(new_dset).shape)\n",
    "    # return torch.stack(new_dset)\n",
    "    return new_dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_reordered = reorder_dataset(train_ds)\n",
    "valid_ds_reordered = reorder_dataset(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dsets(\n",
    "    train_ds_reordered,\n",
    "    valid_ds_reordered,\n",
    "    bs=64, drop_last=True, shuffle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (25 points): Simple NN Language Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a (15 points). The stacked/unrolled representation depict the same 2-layer RNN. Implement this RNN with only:\n",
    "- torch.tensor\n",
    "- tensor functions (from torch.tensor or torch)\n",
    "- torch.nn.Embedding\n",
    "- torch.nn.Linear\n",
    "- torch.nn.relu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"2-layer RNN\" width=\"550\" caption=\"2-layer RNN\" src=\"./att_00025.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"2-layer unrolled RNN\" width=\"550\" caption=\"Two-layer unrolled RNN\" src=\"./att_00026.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACoolRNN(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "        self.h = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for i in range(seq_len):\n",
    "            self.h = self.h + self.i_h(x[:,i])\n",
    "            self.h = F.relu(self.h_h(self.h))\n",
    "            outs.append(self.h_o(self.h))\n",
    "        self.h = self.h.detach()\n",
    "        return torch.stack(outs, dim=1)\n",
    "    \n",
    "    def reset(self): self.h = 0\n",
    "\n",
    "def loss_func(inp, targ):\n",
    "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b (10 points). Find the best learning rate and train your NN with 1cycle policy and Adam (using _Learner_ class from fastai). Report your result with a reasonable metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/10 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/24 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (2048) to match target batch_size (64).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m learner \u001b[39m=\u001b[39m Learner(dls, ACoolRNN(\u001b[39mlen\u001b[39m(vocab), \u001b[39m64\u001b[39m), loss_func\u001b[39m=\u001b[39mloss_func, metrics\u001b[39m=\u001b[39maccuracy, opt_func\u001b[39m=\u001b[39mAdam, cbs\u001b[39m=\u001b[39mModelResetter)\n\u001b[1;32m      4\u001b[0m \u001b[39m# learner = Learner(dls, TwoLayerRNN(seq_len, 512, seq_len), loss_func=nn.CrossEntropyLoss(), metrics=accuracy, opt_func=Adam)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m learner\u001b[39m.\u001b[39;49mfit_one_cycle(\u001b[39m10\u001b[39;49m, \u001b[39m3e-3\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/callback/schedule.py:119\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    116\u001b[0m lr_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([h[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mhypers])\n\u001b[1;32m    117\u001b[0m scheds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[39m/\u001b[39mdiv, lr_max, lr_max\u001b[39m/\u001b[39mdiv_final),\n\u001b[1;32m    118\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mmom\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, \u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoms \u001b[39mif\u001b[39;00m moms \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m moms))}\n\u001b[0;32m--> 119\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(n_epoch, cbs\u001b[39m=\u001b[39;49mParamScheduler(scheds)\u001b[39m+\u001b[39;49mL(cbs), reset_opt\u001b[39m=\u001b[39;49mreset_opt, wd\u001b[39m=\u001b[39;49mwd, start_epoch\u001b[39m=\u001b[39;49mstart_epoch)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/learner.py:256\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mset_hypers(lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39mif\u001b[39;00m lr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m lr)\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch \u001b[39m=\u001b[39m n_epoch\n\u001b[0;32m--> 256\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_fit, \u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelFitException, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_end_cleanup)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/learner.py:245\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch):\n\u001b[1;32m    244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch\u001b[39m=\u001b[39mepoch\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch, \u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelEpochException)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch_train()\n\u001b[1;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/learner.py:231\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch_train\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mtrain\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_batches, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelTrainException)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_batches\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_batch(\u001b[39m*\u001b[39;49mo)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/learner.py:227\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    225\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_device(b)\n\u001b[1;32m    226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split(b)\n\u001b[0;32m--> 227\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_one_batch, \u001b[39m'\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelBatchException)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/fastai/learner.py:208\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mself\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mafter_pred\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myb):\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpred, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49myb)\n\u001b[1;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_grad\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mafter_loss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[91], line 20\u001b[0m, in \u001b[0;36mloss_func\u001b[0;34m(inp, targ)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_func\u001b[39m(inp, targ):\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(inp\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mlen\u001b[39;49m(vocab)), targ\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/gwnlo/lib/python3.10/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (2048) to match target batch_size (64)."
     ]
    }
   ],
   "source": [
    "# Find the best learning rate and train your NN with 1cycle policy and Adam (using Learner class from fastai). Report your result with a reasonable metric\n",
    "\n",
    "learner = Learner(dls, ACoolRNN(len(vocab), 64), loss_func=loss_func, metrics=accuracy, opt_func=Adam, cbs=ModelResetter)\n",
    "# learner = Learner(dls, TwoLayerRNN(seq_len, 512, seq_len), loss_func=nn.CrossEntropyLoss(), metrics=accuracy, opt_func=Adam)\n",
    "learner.fit_one_cycle(10, 3e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (25 points):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a (10 points). Reuse the IMDB reviews dataset from Homework 1. Tokenize (experiment with the vocab size yourself) it with the subword tokenizer you developed in Homework 1 and create _Dataloaders_ in the similar way of Problem 2 to get ready for a LSTM Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b (10 points). Implement a 2-layer LSTM Language Model that has:\n",
    "- One dropout layer (with 50% chance)\n",
    "- Weight tying between the input and output embedding layers\n",
    "- Activation regularization\n",
    "- Temporal activation regularization\n",
    "\n",
    "You can use anything, such as the LSTM module from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers, p_dropout, bs):\n",
    "\n",
    "    def reset(self): \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNRegularizer(Callback):\n",
    "    def __init__(self, alpha=0., beta=0.): self.alpha, self.beta = alpha, beta\n",
    "\n",
    "    # This is called after the forward pass of the model\n",
    "    def after_pred(self):\n",
    "        # Store inside this class the raw output of the model and the dropped out output, respectively \n",
    "        self.raw, self.out = \n",
    "        # Modify the model's output to be just the activation of the last layer\n",
    "        self.learn.pred = \n",
    "\n",
    "    # This is called after the normal loss is computed\n",
    "    def after_loss(self):\n",
    "        if not self.training: return\n",
    "        if self.alpha != 0.:\n",
    "            self.learn.loss += \n",
    "        if self.beta != 0.:\n",
    "            self.learn.loss += "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c (5 points). Find the best learning rate and train your NN with 1cycle policy and Adam. Report your result with a reasonable metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, \n",
    "                # ...\n",
    "                cbs=[ModelResetter, MyRNNRegularizer(alpha=2, beta=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwnlo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af8207cabd4e670ce08470e4167faab7783bfd687a13b41d452e716ef3a3118e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
