{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porblem 1 (25 ponits): Deep Learning Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: You can only use tensors for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('labels.csv'),Path('train'),Path('valid')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [Path('training/0'),Path('training/1'),Path('training/2'),Path('training/3'),Path('training/4'),Path('training/5'),Path('training/6'),Path('training/7'),Path('training/8'),Path('training/9')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'training').ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [Path('testing/0'),Path('testing/1'),Path('testing/2'),Path('testing/3'),Path('testing/4'),Path('testing/5'),Path('testing/6'),Path('testing/7'),Path('testing/8'),Path('testing/9')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'testing').ls().sorted()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a (10 points). Create a 'typical' digit for each by averaging and classify the test data with them. Report your overall accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_distance(a, b): return (a - b).abs().mean((-1, -2))\n",
    "\n",
    "def is_digit(x, mean_digit, other_mean_digit): return mnist_distance(x,mean_digit) < mnist_distance(x,other_mean_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "digits = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "mean_representations = {}\n",
    "\n",
    "for digit in digits:\n",
    "    paths = (path/'training'/digit).ls().sorted()\n",
    "\n",
    "    digit_tensors = [tensor(Image.open(o)) for o in paths]\n",
    "\n",
    "    stacked_digits = torch.stack(digit_tensors).float() / 255\n",
    "\n",
    "    mean_rep = stacked_digits.mean(0)\n",
    "\n",
    "    mean_representations[digit] = mean_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "accs = []\n",
    "for digit in tqdm(digits):\n",
    "    paths = (path/'testing'/digit).ls().sorted()\n",
    "\n",
    "    digit_tensors = torch.stack([tensor(Image.open(o)) for o in paths])\n",
    "\n",
    "    mean_digit = mean_representations[digit]\n",
    "\n",
    "    final_preds = (torch.zeros(digit_tensors.shape[0], dtype=torch.bool) + 1).bool()\n",
    "    for other_digit in digits:\n",
    "\n",
    "        if other_digit == digit: continue\n",
    "\n",
    "        mean_other_digit = mean_representations[other_digit]\n",
    "\n",
    "        preds = is_digit(digit_tensors, mean_digit, mean_other_digit)\n",
    "\n",
    "        final_preds = torch.logical_and(preds, final_preds)\n",
    "    \n",
    "    acc = final_preds.float().mean()\n",
    "    accs.append(acc)\n",
    "    print(\"Accuracy for digit '{}':\\t{}\".format(digit,acc))\n",
    "print(\"Overall ACC: {}\".format(tensor(accs).float().mean()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b (10 points). Create a NN to classify digits (trained with regular SGD). Note that:\n",
    "- Since you deal with 10 categories here, it is preferrable to use cross entropy loss.\n",
    "- Again, anything other than tensors are not allowed in this problem so build your NN from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.data.core.DataLoaders at 0x21dc8929720>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from fastai.vision.all import *\n",
    "import PIL\n",
    "# data loaders\n",
    "# Define the transform to flatten the image\n",
    "class Flatten(Transform):\n",
    "    def encodes(self, x: PIL.Image.Image):\n",
    "        return tensor(np.array(x).flatten())\n",
    "\n",
    "# Create the DataBlock with the Flatten transform\n",
    "datablock = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),\n",
    "                      get_items=get_image_files,\n",
    "                      splitter=RandomSplitter(),\n",
    "                      get_y=parent_label,\n",
    "                      item_tfms=[Resize(28, method=ResizeMethod.Squish, resamples=(1, 1)), Flatten()])\n",
    "\n",
    "# Create the DataLoaders with batch size of 64\n",
    "dls = datablock.dataloaders(path, bs=64)\n",
    "\n",
    "dls.to(torch.device('cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN:\n",
    "    def __init__(self, layers, lr, epochs, optim='sgd', gamma=0.9) -> None:\n",
    "        # self.input_size = input_size\n",
    "        # self.hidden_size = hidden_size\n",
    "        # self.output_size = output_size\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs    \n",
    "\n",
    "        self.device = torch.device('cuda')\n",
    "\n",
    "        self.soft_max = torch.nn.Softmax()\n",
    "\n",
    "        self.eps = 1e-6\n",
    "\n",
    "        if optim == 'sgd':\n",
    "            self.opt = self.sgd\n",
    "        elif optim == 'sgd_mom':\n",
    "            self.opt = self.sgd_momentum\n",
    "        elif optim == 'rms':\n",
    "            self.opt = self.rms_prop\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create random weight tensors for input to hidden and hidden to output layers\n",
    "        # self.weights_input_hidden = torch.randn(input_size, hidden_size, requires_grad=True, device=self.device, dtype=torch.float64)\n",
    "        # self.weights_hidden_output = torch.randn(hidden_size, output_size, requires_grad=True, device=self.device, dtype=torch.float64)\n",
    "\n",
    "        self.weights = {}\n",
    "        self.n_layers = len(layers)\n",
    "        for i in range(len(layers)-1):\n",
    "            self.weights['w'+str(i)] = torch.randn(layers[i], layers[i+1], requires_grad=True, device=self.device, dtype=torch.float64) \n",
    "            self.weights['b'+str(i)] = torch.randn(1, layers[i+1], requires_grad=True, device=self.device, dtype=torch.float64) \n",
    "            \n",
    "            if optim == 'sgd_mom' or optim == 'rms':\n",
    "                self.weights['v'+str(i)] = torch.zeros_like(self.weights['w'+str(i)])\n",
    "\n",
    "    def train(self, dls):\n",
    "        for i in range(self.epochs):\n",
    "            losses = []\n",
    "            print('Starting epoch: {}'.format(i))\n",
    "            for batch in dls.train:\n",
    "                data, labels = batch[0], batch[1]\n",
    "                data = tensor(data, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "                labels = tensor(labels, dtype=torch.uint8, device=self.device,  requires_grad=True)\n",
    "\n",
    "                data = data.to(torch.float64)\n",
    "                labels = labels.to(torch.uint8)\n",
    "                x = self.forward(data)\n",
    "\n",
    "                loss = self.cross_entropy_loss(x, labels)\n",
    "                # self.loss(x, labels)\n",
    "                losses.append(loss.detach().float())\n",
    "                loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    self.opt()\n",
    "            print('\\t loss = {}'.format(tensor(loss).float().mean()))\n",
    "\n",
    "    def sgd(self):\n",
    "        for i in range(self.n_layers - 1):\n",
    "            self.weights['w'+str(i)] -= self.lr * self.weights['w'+str(i)].grad\n",
    "            # self.weights['b'+str(i)] -= self.lr * self.weights['b'+str(i)].grad\n",
    "\n",
    "    def sgd_momentum(self):\n",
    "        for i in range(self.n_layers - 1):\n",
    "            self.weights['v'+str(i)] = self.gamma * self.weights['v'+str(i)] - self.lr * self.weights['w'+str(i)]\n",
    "            self.weights['w'+str(i)] += self.weights['v'+str(i)]\n",
    "            \n",
    "    def rms_prop(self):\n",
    "        for i in range(self.n_layers - 1):\n",
    "            self.weights['v'+str(i)] = self.gamma * self.weights['v'+str(i)] + (1 - self.gamma) * self.weights['w'+str(i)].grad**2\n",
    "            self.weights['w'+str(i)] += (self.lr * self.weights['w'+str(i)].grad) / (torch.sqrt(self.weights['v'+str(i)]+ self.eps) )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # hidden = self.activation(torch.matmul(batch, self.weights_input_hidden))\n",
    "        # output = torch.matmul(hidden, self.weights_hidden_output)\n",
    "        # return output\n",
    "\n",
    "        x = torch.matmul(batch, self.weights['w'+str(0)]) \n",
    "        for i in range(1, self.n_layers-2):\n",
    "            x = torch.matmul(x, self.weights['w'+str(i)]) \n",
    "            # x = x + self.weights['b'+str(i)]\n",
    "            x = torch.add(x, self.weights['b'+str(i)])\n",
    "\n",
    "            x = self.activation(x)\n",
    "\n",
    "        x = torch.matmul(x, self.weights['w'+ str(self.n_layers-2)])\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def cross_entropy_loss(self, predictions, targets):\n",
    "        batch_size = predictions.size(0)\n",
    "        s = self.soft_max(predictions)\n",
    "\n",
    "        log_liklihood = -torch.log(s[range(batch_size), targets.tolist()] + self.eps)\n",
    "        loss = log_liklihood.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MyNN.__init__() got multiple values for argument 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m MyNN(\u001b[39m784\u001b[39;49m, \u001b[39m256\u001b[39;49m, \u001b[39m10\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39mfit_rms_prop(dls)\n",
      "\u001b[1;31mTypeError\u001b[0m: MyNN.__init__() got multiple values for argument 'lr'"
     ]
    }
   ],
   "source": [
    "model = MyNN([784, 256, 128, 64, 10], lr=0.001, epochs=30)\n",
    "\n",
    "model.fit_rms_prop(dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNN([784, 256, 128, 64, 10], lr=0.001, epochs=30, optim='sgd_mom')\n",
    "\n",
    "model.fit_rms_prop(dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNN([784, 256, 128, 64, 10], lr=0.001, epochs=30, optim='rms')\n",
    "\n",
    "model.fit_rms_prop(dls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cant use torch ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=3136, out_features=128, bias=True)\n",
       "  (8): ReLU()\n",
       "  (9): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 7 * 7, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      40.00% [2/5 03:20&lt;05:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.279455</td>\n",
       "      <td>2.277068</td>\n",
       "      <td>0.205429</td>\n",
       "      <td>01:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.239800</td>\n",
       "      <td>2.235144</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='29' class='' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      3.31% [29/875 00:02&lt;01:15 2.2380]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m learn \u001b[39m=\u001b[39m Learner(dls, model, opt_func\u001b[39m=\u001b[39mSGD, loss_func\u001b[39m=\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss(), metrics\u001b[39m=\u001b[39maccuracy)\n\u001b[1;32m----> 2\u001b[0m learn\u001b[39m.\u001b[39;49mfit(\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\learner.py:256\u001b[0m, in \u001b[0;36mLearner.fit\u001b[1;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mset_hypers(lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39mif\u001b[39;00m lr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m lr)\n\u001b[0;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch \u001b[39m=\u001b[39m n_epoch\n\u001b[1;32m--> 256\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_fit, \u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelFitException, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_end_cleanup)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[1;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[0;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\learner.py:245\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch):\n\u001b[0;32m    244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch\u001b[39m=\u001b[39mepoch\n\u001b[1;32m--> 245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch, \u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelEpochException)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[1;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[0;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch_train()\n\u001b[0;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\learner.py:231\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch_train\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mtrain\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_batches, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelTrainException)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[1;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[0;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_batches\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl)\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl): \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_batch(\u001b[39m*\u001b[39mo)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\data\\load.py:127\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbefore_iter()\n\u001b[0;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__idxs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_idxs() \u001b[39m# called in context of main process (not workers/subprocesses)\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m _loaders[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_l\u001b[39m.\u001b[39mnum_workers\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_l):\n\u001b[0;32m    128\u001b[0m     \u001b[39m# pin_memory causes tuples to be converted to lists, so convert them back to tuples\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpin_memory \u001b[39mand\u001b[39;00m \u001b[39mtype\u001b[39m(b) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m: b \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(b)\n\u001b[0;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: b \u001b[39m=\u001b[39m to_device(b, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:39\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_iter)\n\u001b[0;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\data\\load.py:138\u001b[0m, in \u001b[0;36mDataLoader.create_batches\u001b[1;34m(self, samps)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mit \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m    137\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m o:o \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_item, samps))\n\u001b[1;32m--> 138\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunkify(res))\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastcore\\basics.py:230\u001b[0m, in \u001b[0;36mchunked\u001b[1;34m(it, chunk_sz, drop_last, n_chunks)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(it, Iterator): it \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(it)\n\u001b[0;32m    229\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(it, chunk_sz))\n\u001b[0;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mand\u001b[39;00m (\u001b[39mlen\u001b[39m(res)\u001b[39m==\u001b[39mchunk_sz \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m drop_last): \u001b[39myield\u001b[39;00m res\n\u001b[0;32m    232\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(res)\u001b[39m<\u001b[39mchunk_sz: \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\data\\load.py:153\u001b[0m, in \u001b[0;36mDataLoader.do_item\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_item\u001b[39m(\u001b[39mself\u001b[39m, s):\n\u001b[1;32m--> 153\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter_item(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_item(s))\n\u001b[0;32m    154\u001b[0m     \u001b[39mexcept\u001b[39;00m SkipItemException: \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\data\\load.py:160\u001b[0m, in \u001b[0;36mDataLoader.create_item\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_item\u001b[39m(\u001b[39mself\u001b[39m, s):\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindexed: \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[s \u001b[39mor\u001b[39;49;00m \u001b[39m0\u001b[39;49m]\n\u001b[0;32m    161\u001b[0m     \u001b[39melif\u001b[39;00m s \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mit)\n\u001b[0;32m    162\u001b[0m     \u001b[39melse\u001b[39;00m: \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index an iterable dataset numerically - must use `None`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\data\\core.py:455\u001b[0m, in \u001b[0;36mDatasets.__getitem__\u001b[1;34m(self, it)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, it):\n\u001b[1;32m--> 455\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([tl[it] \u001b[39mfor\u001b[39;00m tl \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls])\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m res \u001b[39mif\u001b[39;00m is_indexer(it) \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mres))\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\data\\core.py:455\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, it):\n\u001b[1;32m--> 455\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([tl[it] \u001b[39mfor\u001b[39;00m tl \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls])\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m res \u001b[39mif\u001b[39;00m is_indexer(it) \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mres))\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\data\\core.py:414\u001b[0m, in \u001b[0;36mTfmdLists.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    412\u001b[0m res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(idx)\n\u001b[0;32m    413\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_item \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m res\n\u001b[1;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_after_item(res) \u001b[39mif\u001b[39;00m is_indexer(idx) \u001b[39melse\u001b[39;00m res\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_item)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\data\\core.py:374\u001b[0m, in \u001b[0;36mTfmdLists._after_item\u001b[1;34m(self, o)\u001b[0m\n\u001b[1;32m--> 374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_after_item\u001b[39m(\u001b[39mself\u001b[39m, o): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtfms(o)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastcore\\transform.py:208\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, o)\u001b[0m\n\u001b[1;32m--> 208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, o): \u001b[39mreturn\u001b[39;00m compose_tfms(o, tfms\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs, split_idx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit_idx)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastcore\\transform.py:158\u001b[0m, in \u001b[0;36mcompose_tfms\u001b[1;34m(x, tfms, is_enc, reverse, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m tfms:\n\u001b[0;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_enc: f \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mdecode\n\u001b[1;32m--> 158\u001b[0m     x \u001b[39m=\u001b[39m f(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastcore\\transform.py:81\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mname\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_name\u001b[39m\u001b[39m'\u001b[39m, _get_name(\u001b[39mself\u001b[39m))\n\u001b[1;32m---> 81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m'\u001b[39m\u001b[39mencodes\u001b[39m\u001b[39m'\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m  (\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m'\u001b[39m\u001b[39mdecodes\u001b[39m\u001b[39m'\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__repr__\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39mreturn\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mencodes: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodes\u001b[39m}\u001b[39;00m\u001b[39mdecodes: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodes\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastcore\\transform.py:91\u001b[0m, in \u001b[0;36mTransform._call\u001b[1;34m(self, fn, x, split_idx, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, fn, x, split_idx\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m split_idx\u001b[39m!=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m---> 91\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, fn), x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastcore\\transform.py:97\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[1;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m     96\u001b[0m     ret \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreturns(x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f,\u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     \u001b[39mreturn\u001b[39;00m retain_type(f(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs), x, ret)\n\u001b[0;32m     98\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(f, x_, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m x)\n\u001b[0;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastcore\\dispatch.py:120\u001b[0m, in \u001b[0;36mTypeDispatch.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minst \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: f \u001b[39m=\u001b[39m MethodType(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minst)\n\u001b[0;32m    119\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mowner \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: f \u001b[39m=\u001b[39m MethodType(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mowner)\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\vision\\core.py:123\u001b[0m, in \u001b[0;36mPILBase.create\u001b[1;34m(cls, fn, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fn,ndarray): \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(Image\u001b[39m.\u001b[39mfromarray(fn))\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fn,\u001b[39mbytes\u001b[39m): fn \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(fn)\n\u001b[1;32m--> 123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(load_image(fn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerge(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_open_args, kwargs)))\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\fastai\\vision\\core.py:98\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(fn, mode)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_image\u001b[39m(fn, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     97\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mOpen and load a `PIL.Image` and convert to `mode`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 98\u001b[0m     im \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(fn)\n\u001b[0;32m     99\u001b[0m     im\u001b[39m.\u001b[39mload()\n\u001b[0;32m    100\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39m_new(im\u001b[39m.\u001b[39mim)\n",
      "File \u001b[1;32mc:\\Users\\thomp\\anaconda3\\envs\\gwnlp\\lib\\site-packages\\PIL\\Image.py:3131\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3128\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3130\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3131\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3132\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3134\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn = Learner(dls, model, opt_func=SGD, loss_func=nn.CrossEntropyLoss(), metrics=accuracy)\n",
    "learn.fit(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c (5 points). Implement your own:\n",
    "- SGD with Momemtum\n",
    "- RMSProp\n",
    "\n",
    "by using two more tensors to tracked the smoothed gradients and L2 norms, respectively. Re-train your model on them and report performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD W/ Momentum \n",
    "\n",
    "Code selected from NN class code above:\n",
    "\n",
    "```\n",
    "\n",
    "def fit_sgd_momentum(self, dls, momentum=0.9):\n",
    "        velocity_input = torch.zeros_like(self.weights_input_hidden)\n",
    "        velocity_output = torch.zeros_like(self.weights_hidden_output)\n",
    "\n",
    "        overall_loss = []\n",
    "        for i in range(self.epochs):\n",
    "            losses = []\n",
    "            print('Starting epoch: {}'.format(i))\n",
    "            for batch in dls.train:\n",
    "                data, labels = batch[0], batch[1]\n",
    "                data = tensor(data, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "                labels = tensor(labels, dtype=torch.uint8, device=self.device,  requires_grad=True)\n",
    "\n",
    "                data = data.to(torch.float64)\n",
    "                labels = labels.to(torch.uint8)\n",
    "                x = self.forward(data)\n",
    "\n",
    "                loss = self.cross_entropy_loss(x, labels)\n",
    "                # self.loss(x, labels)\n",
    "                losses.append(loss.detach().float())\n",
    "                overall_loss.append(loss.detach().float())\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    velocity_input = momentum * velocity_input - self.lr * self.weights_input_hidden.grad\n",
    "                    velocity_output = momentum * velocity_output - self.lr * self.weights_hidden_output.grad\n",
    "\n",
    "                    self.weights_input_hidden += velocity_input\n",
    "                    self.weights_hidden_output += velocity_output\n",
    "\n",
    "            print('\\t loss = {}'.format(tensor(loss).float().mean()))\n",
    "        print('\\t overall loss = {}'.format(tensor(overall_loss).float().mean()))\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp\n",
    "\n",
    "Code selected from NN class code above:\n",
    "\n",
    "```\n",
    "def fit_rms_prop(self, dls, decay_rate=0.9):\n",
    "        cache_input = torch.zeros_like(self.weights_input_hidden)\n",
    "        cache_output = torch.zeros_like(self.weights_hidden_output)\n",
    "\n",
    "        overall_loss = []\n",
    "        for i in range(self.epochs):\n",
    "            losses = []\n",
    "            print('Starting epoch: {}'.format(i))\n",
    "            for batch in dls.train:\n",
    "                data, labels = batch[0], batch[1]\n",
    "                data = tensor(data, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "                labels = tensor(labels, dtype=torch.uint8, device=self.device,  requires_grad=True)\n",
    "\n",
    "                data = data.to(torch.float64)\n",
    "                labels = labels.to(torch.uint8)\n",
    "                x = self.forward(data)\n",
    "\n",
    "                loss = self.cross_entropy_loss(x, labels)\n",
    "                # self.loss(x, labels)\n",
    "                losses.append(loss.detach().float())\n",
    "                overall_loss.append(loss.detach().float())\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    cache_input = decay_rate * cache_input + (1 - decay_rate) * self.weights_input_hidden.grad\n",
    "                    cache_output = decay_rate * cache_output + (1 - decay_rate) * self.weights_hidden_output.grad\n",
    "\n",
    "                    self.weights_input_hidden -= (self.lr * self.weights_input_hidden.grad) / (torch.sqrt(cache_input) + self.eps)\n",
    "                    self.weights_hidden_output -= (self.lr * self.weights_hidden_output.grad) / (torch.sqrt(cache_output) + self.eps)\n",
    "\n",
    "            print('\\t loss = {}'.format(tensor(loss).float().mean()))\n",
    "        print('\\t overall loss = {}'.format(tensor(overall_loss).float().mean()))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (25 points): Getting Your Data Ready For a NN Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('valid.txt'),Path('train.txt')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fourteen . fifteen . sixteen . seventeen . eighteen . nineteen . twenty . twenty one . twenty two . twenty three . twenty four . twenty five . twenty six . twenty seven . twenty eight . twenty nine . thirty . thirty one . thirty two . thirty three . thirty four . thirty five . thirty six . thirty seven . thirty eight . thirty nine . forty . forty one . forty two . forty three . forty four . forty five . forty six . forty seven . forty eight . forty nine . fifty . fifty one . fifty two . fifty three . fifty four . fifty five . fifty six . fifty seven . fifty eight . fifty nine . sixty . sixty one . sixty two . sixty three . sixty four . sixty five . sixty six . sixty seven . sixty eight . sixty nine . seventy . seventy one . seventy two . seventy three . seventy four . seventy five . seventy six . seventy seven . seventy eight . seventy nine . eighty . eighty one . eighty two . eighty three'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [line for p in path.ls()[::-1] for line in p.read_text().split(' \\n')]\n",
    "text = ' . '.join([l for l in lines])\n",
    "text[:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a (5 points). Split **text** by space to get **tokens** and create a **vocab** with unique **tokens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \n",
    "vocab = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b (5 points). Create a mapping from word to indices and convert **tokens** into **nums**, which is a list of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = \n",
    "nums = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c (5 points). Create a _dataset_ where each tuple is the input and output of a NN language model. The sequence length of inputs and outputs should be 32. Then create a _training set_ with the first 80% data and a _validation set_ with the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = \n",
    "train_ds = \n",
    "valid_ds ="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d (10 points). Reorder your training set and validation set to be ready for _dataloaders_ to be used in a NN language model. Note that:\n",
    "- `m = len(dset) // 64` batches will be created in the new order without shuffling (the first 64 rows will be the first batch, the next 64 rows will be the second batch, and so on).\n",
    "- The new first 64 rows should be the `1st, (1+m)-th, (1+2m)-th, ..., (1+64m)-th` rows in the original corresponding dataset; The next 64 rows should be the `2nd, (2+m)-th, (2+2m)-th, ..., (2+64m)-th` rows; and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_reordered = \n",
    "valid_ds_reordered ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dsets(\n",
    "    train_ds_reordered,\n",
    "    valid_ds_reordered,\n",
    "    bs=64, drop_last=True, shuffle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (25 points): Simple NN Language Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a (15 points). The stacked/unrolled representation depict the same 2-layer RNN. Implement this RNN with only:\n",
    "- torch.tensor\n",
    "- tensor functions (from torch.tensor or torch)\n",
    "- torch.nn.Embedding\n",
    "- torch.nn.Linear\n",
    "- torch.nn.relu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"2-layer RNN\" width=\"550\" caption=\"2-layer RNN\" src=\"./att_00025.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"2-layer unrolled RNN\" width=\"550\" caption=\"Two-layer unrolled RNN\" src=\"./att_00026.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b (10 points). Find the best learning rate and train your NN with 1cycle policy and Adam (using _Learner_ class from fastai). Report your result with a reasonable metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (25 points):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a (10 points). Reuse the IMDB reviews dataset from Homework 1. Tokenize (experiment with the vocab size yourself) it with the subword tokenizer you developed in Homework 1 and create _Dataloaders_ in the similar way of Problem 2 to get ready for a LSTM Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b (10 points). Implement a 2-layer LSTM Language Model that has:\n",
    "- One dropout layer (with 50% chance)\n",
    "- Weight tying between the input and output embedding layers\n",
    "- Activation regularization\n",
    "- Temporal activation regularization\n",
    "\n",
    "You can use anything, such as the LSTM module from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers, p_dropout, bs):\n",
    "\n",
    "    def reset(self): \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNRegularizer(Callback):\n",
    "    def __init__(self, alpha=0., beta=0.): self.alpha, self.beta = alpha, beta\n",
    "\n",
    "    # This is called after the forward pass of the model\n",
    "    def after_pred(self):\n",
    "        # Store inside this class the raw output of the model and the dropped out output, respectively \n",
    "        self.raw, self.out = \n",
    "        # Modify the model's output to be just the activation of the last layer\n",
    "        self.learn.pred = \n",
    "\n",
    "    # This is called after the normal loss is computed\n",
    "    def after_loss(self):\n",
    "        if not self.training: return\n",
    "        if self.alpha != 0.:\n",
    "            self.learn.loss += \n",
    "        if self.beta != 0.:\n",
    "            self.learn.loss += "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c (5 points). Find the best learning rate and train your NN with 1cycle policy and Adam. Report your result with a reasonable metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, \n",
    "                # ...\n",
    "                cbs=[ModelResetter, MyRNNRegularizer(alpha=2, beta=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3678fccd7bdf249d4915ccf874334202039756271843e55b69924f5b9bf0c5ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
